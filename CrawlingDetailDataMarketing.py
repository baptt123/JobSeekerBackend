import time
import pandas as pd
import os
import mysql.connector
from mysql.connector import Error
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from datetime import datetime

# ==============================================================================
# PH·∫¶N L√ÄM VI·ªÜC V·ªöI DATABASE (THAY TH·∫æ CSV)
# ==============================================================================

# C·∫•u h√¨nh k·∫øt n·ªëi database
DB_CONFIG = {
    'host': 'localhost',
    'database': 'job_finder_app',
    'user': 'root',  # Thay ƒë·ªïi username c·ªßa b·∫°n
    'password': '',  # Thay ƒë·ªïi password c·ªßa b·∫°n
    'port': 3306
}

def create_database_connection():
    """T·∫°o k·∫øt n·ªëi ƒë·∫øn MySQL database."""
    try:
        connection = mysql.connector.connect(**DB_CONFIG)
        if connection.is_connected():
            # print("‚úÖ K·∫øt n·ªëi database th√†nh c√¥ng!") # T·∫Øt b·ªõt log cho g·ªçn
            return connection
    except Error as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi database: {e}")
        return None

def setup_database_tables():
    """T·∫°o c√°c b·∫£ng c·∫ßn thi·∫øt n·∫øu ch∆∞a t·ªìn t·∫°i."""
    connection = create_database_connection()
    if not connection:
        return False

    try:
        cursor = connection.cursor()

        # T·∫°o b·∫£ng job_categories n·∫øu ch∆∞a c√≥
        cursor.execute("""
                       CREATE TABLE IF NOT EXISTS job_categories (
                                                                     id INT PRIMARY KEY AUTO_INCREMENT,
                                                                     name VARCHAR(100) UNIQUE
                           )
                       """)

        # Th√™m danh m·ª•c IT m·∫∑c ƒë·ªãnh
        cursor.execute("""
                       INSERT IGNORE INTO job_categories (name) VALUES ('IT Software')
                       """)

        # T·∫°o b·∫£ng users v·ªõi role employer m·∫∑c ƒë·ªãnh cho c√°c c√¥ng ty
        cursor.execute("""
                       CREATE TABLE IF NOT EXISTS users (
                                                            id INT PRIMARY KEY AUTO_INCREMENT,
                                                            name VARCHAR(100),
                           company_name VARCHAR(150),
                           email VARCHAR(100) UNIQUE,
                           password VARCHAR(100),
                           phone VARCHAR(20),
                           role ENUM('user', 'employer', 'admin') DEFAULT 'employer',
                           avatar_url TEXT,
                           created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                           )
                       """)

        # T·∫†O B·∫¢NG JOBS - ƒê√É S·ª¨A L·∫†I ƒê·ªÇ ƒê·∫¢M B·∫¢O C√ì ƒê·ª¶ C·ªòT
        cursor.execute("""
                       CREATE TABLE IF NOT EXISTS jobs (
                                                           id INT PRIMARY KEY AUTO_INCREMENT,
                                                           employer_id INT,
                                                           title VARCHAR(150),
                           description TEXT,
                           location VARCHAR(100),
                           salary_range VARCHAR(50),
                           requirements TEXT,
                           benefits TEXT,
                           category_id INT,
                           status ENUM('open', 'closed') DEFAULT 'open',
                           created_at DATETIME DEFAULT CURRENT_TIMESTAMP,

                           -- Th√™m c√°c tr∆∞·ªùng t·ª´ crawl
                           company_name VARCHAR(150),
                           logo_url TEXT,
                           job_url TEXT UNIQUE,
                           crawl_page INT,
                           job_level VARCHAR(50),
                           experience_required VARCHAR(50),
                           quantity_needed VARCHAR(20),
                           work_type VARCHAR(50),
                           application_deadline DATE,

                           -- D·ªØ li·ªáu m·ªü r·ªông t·ª´ wp-container
                           wp_full_content LONGTEXT,
                           wp_headings TEXT,
                           wp_paragraphs LONGTEXT,
                           wp_lists LONGTEXT,
                           wp_divs_content LONGTEXT,
                           wp_important_info TEXT,
                           wp_links TEXT,

                           -- Th√¥ng tin c√¥ng ty chi ti·∫øt
                           company_website VARCHAR(200),
                           company_address TEXT,
                           company_description LONGTEXT,
                           company_size VARCHAR(100),
                           company_industry VARCHAR(100),
                           company_founded VARCHAR(50),
                           company_email VARCHAR(100),
                           company_phone VARCHAR(50),
                           company_logo_url TEXT,
                           company_full_content LONGTEXT,

                           FOREIGN KEY (employer_id) REFERENCES users(id),
                           FOREIGN KEY (category_id) REFERENCES job_categories(id)
                           )
                       """)

        connection.commit()
        print("‚úÖ Database tables ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p th√†nh c√¥ng!")
        return True

    except Error as e:
        print(f"‚ùå L·ªói khi thi·∫øt l·∫≠p database: {e}")
        return False
    finally:
        if connection.is_connected():
            cursor.close()
            connection.close()

# def save_company_to_db(company_details, company_name):
#     """L∆∞u th√¥ng tin c√¥ng ty v√†o database v√† tr·∫£ v·ªÅ employer_id."""
#     connection = create_database_connection()
#     if not connection:
#         return None
#
#     try:
#         cursor = connection.cursor()
#
#         # Ki·ªÉm tra xem c√¥ng ty ƒë√£ t·ªìn t·∫°i ch∆∞a
#         cursor.execute("SELECT id FROM users WHERE company_name = %s AND role = 'employer'", (company_name,))
#         result = cursor.fetchone()
#
#         if result:
#             # print(f"‚úÖ C√¥ng ty '{company_name}' ƒë√£ t·ªìn t·∫°i v·ªõi ID: {result[0]}") # T·∫Øt b·ªõt log cho g·ªçn
#             return result[0]
#
#         # Th√™m c√¥ng ty m·ªõi
#         insert_query = """
#                        INSERT INTO users (company_name, name, email, phone, role, avatar_url, created_at)
#                        VALUES (%s, %s, %s, %s, 'employer', %s, %s)
#                        """
#
#         company_data = (
#             company_name,
#             company_details.get('name', company_name),
#             company_details.get('email', None),
#             company_details.get('phone', None),
#             company_details.get('logo_url', None),
#             datetime.now()
#         )
#
#         cursor.execute(insert_query, company_data)
#         connection.commit()
#
#         employer_id = cursor.lastrowid
#         print(f"‚úÖ ƒê√£ th√™m c√¥ng ty m·ªõi '{company_name}' v·ªõi ID: {employer_id}")
#         return employer_id
#
#     except Error as e:
#         print(f"‚ùå L·ªói khi l∆∞u c√¥ng ty: {e}")
#         return None
#     finally:
#         if connection.is_connected():
#             cursor.close()
#             connection.close()


def save_company_to_db(company_details, company_name):
    """L∆∞u th√¥ng tin c√¥ng ty v√†o database v√† tr·∫£ v·ªÅ employer_id - ENHANCED VERSION."""
    connection = create_database_connection()
    if not connection:
        return None

    try:
        cursor = connection.cursor()

        # Ki·ªÉm tra xem c√¥ng ty ƒë√£ t·ªìn t·∫°i ch∆∞a
        cursor.execute("SELECT id FROM users WHERE company_name = %s AND role = 'employer'", (company_name,))
        result = cursor.fetchone()

        if result:
            # print(f"‚úÖ C√¥ng ty '{company_name}' ƒë√£ t·ªìn t·∫°i v·ªõi ID: {result[0]}") # T·∫Øt b·ªõt log cho g·ªçn

            # C·∫¨P NH·∫¨T TH√îNG TIN C√îNG TY N·∫æU ƒê√É T·ªíN T·∫†I V√Ä C√ì D·ªÆ LI·ªÜU M·ªöI
            if company_details:
                update_fields = []
                update_values = []

                # Ch·ªâ c·∫≠p nh·∫≠t nh·ªØng tr∆∞·ªùng c√≥ d·ªØ li·ªáu m·ªõi
                if company_details.get('name') and company_details['name'] != company_name:
                    update_fields.append("name = %s")
                    update_values.append(company_details['name'])

                if company_details.get('email'):
                    update_fields.append("email = %s")
                    update_values.append(company_details['email'])

                if company_details.get('phone'):
                    update_fields.append("phone = %s")
                    update_values.append(company_details['phone'])

                if company_details.get('logo_url'):
                    update_fields.append("avatar_url = %s")
                    update_values.append(company_details['logo_url'])

                # Th·ª±c hi·ªán update n·∫øu c√≥ d·ªØ li·ªáu m·ªõi
                if update_fields:
                    update_query = f"UPDATE users SET {', '.join(update_fields)} WHERE id = %s"
                    update_values.append(result[0])
                    cursor.execute(update_query, tuple(update_values))
                    connection.commit()
                    print(f"üîÑ ƒê√£ c·∫≠p nh·∫≠t th√¥ng tin c√¥ng ty '{company_name}' v·ªõi ID: {result[0]}")

            return result[0]

        # TH√äM C√îNG TY M·ªöI V·ªöI TH√îNG TIN CHI TI·∫æT
        insert_query = """
                       INSERT INTO users (company_name, name, email, phone, role, avatar_url, created_at)
                       VALUES (%s, %s, %s, %s, 'employer', %s, %s)
                       """

        # L·∫•y th√¥ng tin t·ª´ company_details v·ªõi fallback values
        company_data = (
            company_name,
            company_details.get('name', company_name) if company_details else company_name,
            company_details.get('email', None) if company_details else None,
            company_details.get('phone', None) if company_details else None,
            company_details.get('logo_url', None) if company_details else None,
            datetime.now()
        )

        cursor.execute(insert_query, company_data)
        connection.commit()

        employer_id = cursor.lastrowid

        # Log th√¥ng tin chi ti·∫øt khi th√™m c√¥ng ty m·ªõi
        added_info = []
        if company_details:
            if company_details.get('email'):
                added_info.append(f"email: {company_details['email']}")
            if company_details.get('phone'):
                added_info.append(f"phone: {company_details['phone']}")
            if company_details.get('website'):
                added_info.append(f"website: {company_details['website'][:30]}...")
            if company_details.get('logo_url'):
                added_info.append(f"logo: c√≥")
            if company_details.get('description'):
                added_info.append(f"m√¥ t·∫£: {len(company_details['description'])} k√Ω t·ª±")

        if added_info:
            print(f"‚úÖ ƒê√£ th√™m c√¥ng ty m·ªõi '{company_name}' v·ªõi ID: {employer_id}")
            print(f"   üìã Th√¥ng tin b·ªï sung: {' | '.join(added_info)}")
        else:
            print(f"‚úÖ ƒê√£ th√™m c√¥ng ty m·ªõi '{company_name}' v·ªõi ID: {employer_id} (th√¥ng tin c∆° b·∫£n)")

        return employer_id

    except Error as e:
        print(f"‚ùå L·ªói khi l∆∞u c√¥ng ty: {e}")
        return None
    finally:
        if connection.is_connected():
            cursor.close()
            connection.close()



# def save_job_to_db(job_data):
#     """L∆∞u m·ªôt c√¥ng vi·ªác v√†o database."""
#     connection = create_database_connection()
#     if not connection:
#         return False
#
#     try:
#         cursor = connection.cursor()
#
#         # L·∫•y category_id cho IT Software
#         cursor.execute("SELECT id FROM job_categories WHERE name = 'IT Software'")
#         category_result = cursor.fetchone()
#         category_id = category_result[0] if category_result else 1
#
#         # L∆∞u th√¥ng tin c√¥ng ty v√† l·∫•y employer_id
#         company_details = job_data.get("Company Details", {})
#         company_name = job_data.get("C√¥ng ty", "Unknown Company")
#         employer_id = save_company_to_db(company_details, company_name)
#
#         if not employer_id:
#             employer_id = None  # S·∫Ω ƒë·ªÉ NULL trong database
#
#         # Chu·∫©n b·ªã d·ªØ li·ªáu job
#         application_deadline = None
#         deadline_str = job_data.get("H·∫°n n·ªôp h·ªì s∆°", "")
#         if deadline_str and deadline_str != "Kh√¥ng c√≥ th√¥ng tin":
#             try:
#                 # Th·ª≠ parse ng√†y th√°ng (c√≥ th·ªÉ c·∫ßn ƒëi·ªÅu ch·ªânh format)
#                 application_deadline = datetime.strptime(deadline_str, "%d/%m/%Y").date()
#             except:
#                 pass
#
#         # S·ª¨A L·∫†I QUERY - B·ªï sung c√°c tr∆∞·ªùng v√†o ON DUPLICATE KEY UPDATE
#         insert_query = """
#                        INSERT INTO jobs (
#                            employer_id, title, description, location, salary_range, requirements, benefits,
#                            category_id, company_name, logo_url, job_url, crawl_page,
#                            job_level, experience_required, quantity_needed, work_type, application_deadline,
#                            wp_full_content, wp_headings, wp_paragraphs, wp_lists, wp_divs_content,
#                            wp_important_info, wp_links,
#                            company_website, company_address, company_description, company_size,
#                            company_industry, company_founded, company_email, company_phone,
#                            company_logo_url, company_full_content, created_at
#                        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
#                            ON DUPLICATE KEY UPDATE
#                                                 title = VALUES(title),
#                                                 description = VALUES(description),
#                                                 location = VALUES(location),
#                                                 salary_range = VALUES(salary_range),
#                                                 requirements = VALUES(requirements),
#                                                 benefits = VALUES(benefits),
#                                                 company_name = VALUES(company_name),
#                                                 logo_url = VALUES(logo_url),
#                                                 job_level = VALUES(job_level),
#                                                 experience_required = VALUES(experience_required),
#                                                 application_deadline = VALUES(application_deadline),
#                                                 company_description = VALUES(company_description)
#                        """
#
#         job_values = (
#             employer_id,
#             job_data.get("Ti√™u ƒë·ªÅ", "")[:150],
#             job_data.get("M√¥ t·∫£ c√¥ng vi·ªác", ""),
#             job_data.get("ƒê·ªãa ƒëi·ªÉm", "")[:100],
#             job_data.get("M·ª©c l∆∞∆°ng", "")[:50],
#             job_data.get("Y√™u c·∫ßu c√¥ng vi·ªác", ""),
#             job_data.get("Quy·ªÅn l·ª£i", ""), # D·ªØ li·ªáu cho c·ªôt `benefits`
#             category_id,
#             company_name[:150],
#             job_data.get("Logo URL", ""), # D·ªØ li·ªáu cho c·ªôt `logo_url`
#             job_data.get("Link", ""),
#             job_data.get("Trang", 0),
#             job_data.get("C·∫•p b·∫≠c", "")[:50],
#             job_data.get("Kinh nghi·ªám", "")[:50],
#             job_data.get("S·ªë l∆∞·ª£ng tuy·ªÉn", "")[:20],
#             job_data.get("H√¨nh th·ª©c", "")[:50],
#             application_deadline,
#             job_data.get("To√†n b·ªô n·ªôi dung WP", ""),
#             job_data.get("C√°c ti√™u ƒë·ªÅ", ""),
#             job_data.get("C√°c ƒëo·∫°n vƒÉn", ""),
#             job_data.get("C√°c danh s√°ch", ""),
#             job_data.get("N·ªôi dung c√°c div", ""),
#             job_data.get("Th√¥ng tin quan tr·ªçng", ""),
#             job_data.get("C√°c li√™n k·∫øt", ""),
#             company_details.get("website", "")[:200],
#             company_details.get("address", ""),
#             company_details.get("description", ""),
#             company_details.get("size", "")[:100],
#             company_details.get("industry", "")[:100],
#             company_details.get("founded", "")[:50],
#             company_details.get("email", "")[:100],
#             company_details.get("phone", "")[:50],
#             company_details.get("logo_url", ""),
#             company_details.get("full_content", ""),
#             datetime.now()
#         )
#
#         cursor.execute(insert_query, job_values)
#         connection.commit()
#
#         job_id = cursor.lastrowid
#         if job_id == 0: # Job ƒë√£ t·ªìn t·∫°i v√† ƒë∆∞·ª£c update
#             # print(f"üîÑ ƒê√£ c·∫≠p nh·∫≠t job: {job_data.get('Ti√™u ƒë·ªÅ', 'Unknown')}")
#             pass
#         else:
#             print(f"‚úÖ ƒê√£ l∆∞u job m·ªõi '{job_data.get('Ti√™u ƒë·ªÅ', 'Unknown')}' v·ªõi ID: {job_id}")
#         return True
#
#     except Error as e:
#         print(f"‚ùå L·ªói khi l∆∞u job v√†o database: {e}")
#         return False
#     finally:
#         if connection.is_connected():
#             cursor.close()
#             connection.close()


def get_table_columns(connection, table_name):
    """L·∫•y danh s√°ch c√°c c·ªôt c√≥ trong b·∫£ng."""
    try:
        cursor = connection.cursor()
        cursor.execute(f"DESCRIBE {table_name}")
        columns = [row[0] for row in cursor.fetchall()]
        return columns
    except Error as e:
        print(f"‚ùå L·ªói khi l·∫•y c·∫•u tr√∫c b·∫£ng {table_name}: {e}")
        return []

def save_job_to_db(job_data):
    """L∆∞u m·ªôt c√¥ng vi·ªác v√†o database - PHI√äN B·∫¢N T∆Ø∆†NG TH√çCH."""
    connection = create_database_connection()
    if not connection:
        return False

    try:
        cursor = connection.cursor()

        # L·∫•y danh s√°ch c√°c c·ªôt c√≥ trong b·∫£ng jobs
        available_columns = get_table_columns(connection, 'jobs')
        if not available_columns:
            print("‚ùå Kh√¥ng th·ªÉ l·∫•y c·∫•u tr√∫c b·∫£ng jobs")
            return False

        # L·∫•y category_id cho IT Software
        cursor.execute("SELECT id FROM job_categories WHERE name = 'IT Software'")
        category_result = cursor.fetchone()
        category_id = category_result[0] if category_result else 1

        # L∆∞u th√¥ng tin c√¥ng ty v√† l·∫•y employer_id
        company_details = job_data.get("Company Details", {})
        company_name = job_data.get("C√¥ng ty", "Unknown Company")
        employer_id = save_company_to_db(company_details, company_name)

        if not employer_id:
            employer_id = None  # S·∫Ω ƒë·ªÉ NULL trong database

        # Chu·∫©n b·ªã d·ªØ li·ªáu job
        application_deadline = None
        deadline_str = job_data.get("H·∫°n n·ªôp h·ªì s∆°", "")
        if deadline_str and deadline_str != "Kh√¥ng c√≥ th√¥ng tin":
            try:
                application_deadline = datetime.strptime(deadline_str, "%d/%m/%Y").date()
            except:
                pass

        # Mapping d·ªØ li·ªáu v·ªõi ki·ªÉm tra c·ªôt c√≥ t·ªìn t·∫°i
        job_fields = {
            'employer_id': employer_id,
            'title': job_data.get("Ti√™u ƒë·ªÅ", "")[:150],
            'description': job_data.get("M√¥ t·∫£ c√¥ng vi·ªác", ""),
            'location': job_data.get("ƒê·ªãa ƒëi·ªÉm", "")[:100],
            'salary_range': job_data.get("M·ª©c l∆∞∆°ng", "")[:50],
            'requirements': job_data.get("Y√™u c·∫ßu c√¥ng vi·ªác", ""),
            'benefits': job_data.get("Quy·ªÅn l·ª£i", ""),  # C√≥ th·ªÉ kh√¥ng t·ªìn t·∫°i
            'category_id': category_id,
            'company_name': company_name[:150],
            'logo_url': job_data.get("Logo URL", ""),  # C√≥ th·ªÉ kh√¥ng t·ªìn t·∫°i
            'job_url': job_data.get("Link", ""),
            'crawl_page': job_data.get("Trang", 0),
            'job_level': job_data.get("C·∫•p b·∫≠c", "")[:50],
            'experience_required': job_data.get("Kinh nghi·ªám", "")[:50],
            'quantity_needed': job_data.get("S·ªë l∆∞·ª£ng tuy·ªÉn", "")[:20],
            'work_type': job_data.get("H√¨nh th·ª©c", "")[:50],
            'application_deadline': application_deadline,
            'wp_full_content': job_data.get("To√†n b·ªô n·ªôi dung WP", ""),
            'wp_headings': job_data.get("C√°c ti√™u ƒë·ªÅ", ""),
            'wp_paragraphs': job_data.get("C√°c ƒëo·∫°n vƒÉn", ""),
            'wp_lists': job_data.get("C√°c danh s√°ch", ""),
            'wp_divs_content': job_data.get("N·ªôi dung c√°c div", ""),
            'wp_important_info': job_data.get("Th√¥ng tin quan tr·ªçng", ""),
            'wp_links': job_data.get("C√°c li√™n k·∫øt", ""),
            'company_website': company_details.get("website", "")[:200],
            'company_address': company_details.get("address", ""),
            'company_description': company_details.get("description", ""),
            'company_size': company_details.get("size", "")[:100],
            'company_industry': company_details.get("industry", "")[:100],
            'company_founded': company_details.get("founded", "")[:50],
            'company_email': company_details.get("email", "")[:100],
            'company_phone': company_details.get("phone", "")[:50],
            'company_logo_url': company_details.get("logo_url", ""),
            'company_full_content': company_details.get("full_content", ""),
            'created_at': datetime.now()
        }

        # Ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt c√≥ trong b·∫£ng
        valid_fields = {}
        valid_values = []

        for field, value in job_fields.items():
            if field in available_columns:
                valid_fields[field] = value
                valid_values.append(value)
            else:
                print(f"‚ö†Ô∏è B·ªè qua c·ªôt kh√¥ng t·ªìn t·∫°i: {field}")

        if not valid_fields:
            print("‚ùå Kh√¥ng c√≥ c·ªôt h·ª£p l·ªá n√†o ƒë·ªÉ insert")
            return False

        # T·∫°o query ƒë·ªông
        columns = list(valid_fields.keys())
        placeholders = ', '.join(['%s'] * len(columns))
        columns_str = ', '.join(columns)

        # T·∫°o ON DUPLICATE KEY UPDATE cho c√°c c·ªôt c∆° b·∫£n
        basic_update_fields = []
        for field in ['title', 'description', 'location', 'salary_range', 'requirements', 'company_name']:
            if field in available_columns:
                basic_update_fields.append(f"{field} = VALUES({field})")

        update_clause = ', '.join(basic_update_fields) if basic_update_fields else "title = VALUES(title)"

        insert_query = f"""
        INSERT INTO jobs ({columns_str}) VALUES ({placeholders})
        ON DUPLICATE KEY UPDATE {update_clause}
        """

        cursor.execute(insert_query, valid_values)
        connection.commit()

        job_id = cursor.lastrowid
        if job_id == 0:
            # Job ƒë√£ t·ªìn t·∫°i v√† ƒë∆∞·ª£c update
            pass
        else:
            print(f"‚úÖ ƒê√£ l∆∞u job m·ªõi '{job_data.get('Ti√™u ƒë·ªÅ', 'Unknown')}' v·ªõi ID: {job_id}")

        return True

    except Error as e:
        print(f"‚ùå L·ªói khi l∆∞u job v√†o database: {e}")
        print(f"üìã C·ªôt c√≥ s·∫µn trong b·∫£ng: {available_columns}")
        return False
    finally:
        if connection.is_connected():
            cursor.close()
            connection.close()


def setup_database_tables():
    """T·∫°o c√°c b·∫£ng c·∫ßn thi·∫øt n·∫øu ch∆∞a t·ªìn t·∫°i - PHI√äN B·∫¢N C·∫¢I TI·∫æN."""
    connection = create_database_connection()
    if not connection:
        return False

    try:
        cursor = connection.cursor()

        # T·∫°o b·∫£ng job_categories n·∫øu ch∆∞a c√≥
        cursor.execute("""
                       CREATE TABLE IF NOT EXISTS job_categories (
                                                                     id INT PRIMARY KEY AUTO_INCREMENT,
                                                                     name VARCHAR(100) UNIQUE
                           )
                       """)

        # Th√™m danh m·ª•c IT m·∫∑c ƒë·ªãnh
        cursor.execute("""
                       INSERT IGNORE INTO job_categories (name) VALUES ('IT Software')
                       """)

        # T·∫°o b·∫£ng users v·ªõi role employer m·∫∑c ƒë·ªãnh cho c√°c c√¥ng ty
        cursor.execute("""
                       CREATE TABLE IF NOT EXISTS users (
                                                            id INT PRIMARY KEY AUTO_INCREMENT,
                                                            name VARCHAR(100),
                           company_name VARCHAR(150),
                           email VARCHAR(100) UNIQUE,
                           password VARCHAR(100),
                           phone VARCHAR(20),
                           role ENUM('user', 'employer', 'admin') DEFAULT 'employer',
                           avatar_url TEXT,
                           created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                           )
                       """)

        # Ki·ªÉm tra xem b·∫£ng jobs ƒë√£ t·ªìn t·∫°i ch∆∞a
        cursor.execute("SHOW TABLES LIKE 'jobs'")
        table_exists = cursor.fetchone() is not None

        if table_exists:
            # B·∫£ng ƒë√£ t·ªìn t·∫°i, ki·ªÉm tra v√† th√™m c√°c c·ªôt thi·∫øu
            print("üîç B·∫£ng 'jobs' ƒë√£ t·ªìn t·∫°i. ƒêang ki·ªÉm tra c·∫•u tr√∫c...")

            # L·∫•y danh s√°ch c·ªôt hi·ªán c√≥
            cursor.execute("DESCRIBE jobs")
            existing_columns = [row[0] for row in cursor.fetchall()]
            print(f"üìã C√°c c·ªôt hi·ªán c√≥: {existing_columns}")

            # Danh s√°ch t·∫•t c·∫£ c√°c c·ªôt c·∫ßn c√≥
            required_columns = {
                'benefits': 'TEXT',
                'logo_url': 'TEXT',
                'job_level': 'VARCHAR(50)',
                'experience_required': 'VARCHAR(50)',
                'quantity_needed': 'VARCHAR(20)',
                'work_type': 'VARCHAR(50)',
                'application_deadline': 'DATE',
                'wp_full_content': 'LONGTEXT',
                'wp_headings': 'TEXT',
                'wp_paragraphs': 'LONGTEXT',
                'wp_lists': 'LONGTEXT',
                'wp_divs_content': 'LONGTEXT',
                'wp_important_info': 'TEXT',
                'wp_links': 'TEXT',
                'company_website': 'VARCHAR(200)',
                'company_address': 'TEXT',
                'company_description': 'LONGTEXT',
                'company_size': 'VARCHAR(100)',
                'company_industry': 'VARCHAR(100)',
                'company_founded': 'VARCHAR(50)',
                'company_email': 'VARCHAR(100)',
                'company_phone': 'VARCHAR(50)',
                'company_logo_url': 'TEXT',
                'company_full_content': 'LONGTEXT'
            }

            # Th√™m c√°c c·ªôt thi·∫øu
            added_columns = []
            for column, column_type in required_columns.items():
                if column not in existing_columns:
                    try:
                        cursor.execute(f"ALTER TABLE jobs ADD COLUMN {column} {column_type}")
                        added_columns.append(column)
                        print(f"‚úÖ ƒê√£ th√™m c·ªôt: {column} ({column_type})")
                    except Error as e:
                        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ th√™m c·ªôt {column}: {e}")

            if added_columns:
                print(f"üîß ƒê√£ th√™m {len(added_columns)} c·ªôt m·ªõi v√†o b·∫£ng 'jobs'")
            else:
                print("‚úÖ T·∫•t c·∫£ c·ªôt ƒë√£ t·ªìn t·∫°i")

        else:
            # T·∫°o b·∫£ng m·ªõi v·ªõi ƒë·∫ßy ƒë·ªß c·ªôt
            print("üÜï T·∫°o b·∫£ng 'jobs' m·ªõi...")
            cursor.execute("""
                           CREATE TABLE jobs (
                                                 id INT PRIMARY KEY AUTO_INCREMENT,
                                                 employer_id INT,
                                                 title VARCHAR(150),
                                                 description TEXT,
                                                 location VARCHAR(100),
                                                 salary_range VARCHAR(50),
                                                 requirements TEXT,
                                                 benefits TEXT,
                                                 category_id INT,
                                                 status ENUM('open', 'closed') DEFAULT 'open',
                                                 created_at DATETIME DEFAULT CURRENT_TIMESTAMP,

                               -- Th√™m c√°c tr∆∞·ªùng t·ª´ crawl
                                                 company_name VARCHAR(150),
                                                 logo_url TEXT,
                                                 job_url TEXT UNIQUE,
                                                 crawl_page INT,
                                                 job_level VARCHAR(50),
                                                 experience_required VARCHAR(50),
                                                 quantity_needed VARCHAR(20),
                                                 work_type VARCHAR(50),
                                                 application_deadline DATE,

                               -- D·ªØ li·ªáu m·ªü r·ªông t·ª´ wp-container
                                                 wp_full_content LONGTEXT,
                                                 wp_headings TEXT,
                                                 wp_paragraphs LONGTEXT,
                                                 wp_lists LONGTEXT,
                                                 wp_divs_content LONGTEXT,
                                                 wp_important_info TEXT,
                                                 wp_links TEXT,

                               -- Th√¥ng tin c√¥ng ty chi ti·∫øt
                                                 company_website VARCHAR(200),
                                                 company_address TEXT,
                                                 company_description LONGTEXT,
                                                 company_size VARCHAR(100),
                                                 company_industry VARCHAR(100),
                                                 company_founded VARCHAR(50),
                                                 company_email VARCHAR(100),
                                                 company_phone VARCHAR(50),
                                                 company_logo_url TEXT,
                                                 company_full_content LONGTEXT,

                                                 FOREIGN KEY (employer_id) REFERENCES users(id),
                                                 FOREIGN KEY (category_id) REFERENCES job_categories(id)
                           )
                           """)
            print("‚úÖ ƒê√£ t·∫°o b·∫£ng 'jobs' m·ªõi v·ªõi ƒë·∫ßy ƒë·ªß c·ªôt")

        connection.commit()
        print("‚úÖ Database tables ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p th√†nh c√¥ng!")
        return True

    except Error as e:
        print(f"‚ùå L·ªói khi thi·∫øt l·∫≠p database: {e}")
        return False
    finally:
        if connection.is_connected():
            cursor.close()
            connection.close()


# Thay th·∫ø h√†m c≈© b·∫±ng h√†m m·ªõi trong code
def get_database_stats():
    """Hi·ªÉn th·ªã th·ªëng k√™ database - PHI√äN B·∫¢N C·∫¢I TI·∫æN."""
    connection = create_database_connection()
    if not connection:
        return

    try:
        cursor = connection.cursor()

        print("\nüìä TH·ªêNG K√ä DATABASE:")
        print("="*50)

        # Th·ªëng k√™ jobs
        cursor.execute("SELECT COUNT(*) FROM jobs")
        total_jobs = cursor.fetchone()[0]
        print(f"üìù T·ªïng s·ªë c√¥ng vi·ªác: {total_jobs}")

        # Th·ªëng k√™ companies
        cursor.execute("SELECT COUNT(*) FROM users WHERE role = 'employer'")
        total_companies = cursor.fetchone()[0]
        print(f"üè¢ T·ªïng s·ªë c√¥ng ty: {total_companies}")

        # Ki·ªÉm tra c·ªôt logo_url c√≥ t·ªìn t·∫°i kh√¥ng
        available_columns = get_table_columns(connection, 'jobs')

        if 'logo_url' in available_columns:
            cursor.execute("SELECT COUNT(*) FROM jobs WHERE logo_url IS NOT NULL AND logo_url != ''")
            jobs_with_logo = cursor.fetchone()[0]
            print(f"üñºÔ∏è  Jobs c√≥ logo: {jobs_with_logo} / {total_jobs}")
        else:
            print(f"‚ö†Ô∏è  C·ªôt 'logo_url' ch∆∞a t·ªìn t·∫°i trong b·∫£ng")

        if 'company_description' in available_columns:
            cursor.execute("SELECT COUNT(*) FROM jobs WHERE company_description IS NOT NULL AND company_description != ''")
            jobs_with_company_info = cursor.fetchone()[0]
            print(f"üìã Jobs c√≥ th√¥ng tin c√¥ng ty: {jobs_with_company_info} / {total_jobs}")

        # Top companies
        cursor.execute("""
                       SELECT company_name, COUNT(*) as job_count
                       FROM jobs
                       WHERE company_name IS NOT NULL AND company_name != '' AND company_name != 'Kh√¥ng x√°c ƒë·ªãnh'
                       GROUP BY company_name
                       ORDER BY job_count DESC
                           LIMIT 5
                       """)
        top_companies = cursor.fetchall()
        if top_companies:
            print(f"\nüèÜ TOP 5 C√îNG TY C√ì NHI·ªÄU VI·ªÜC NH·∫§T:")
            for company, count in top_companies:
                print(f"   - {company}: {count} vi·ªác l√†m")

        # Top locations
        cursor.execute("""
                       SELECT location, COUNT(*) as job_count
                       FROM jobs
                       WHERE location IS NOT NULL AND location != '' AND location != 'Kh√¥ng x√°c ƒë·ªãnh'
                       GROUP BY location
                       ORDER BY job_count DESC
                           LIMIT 5
                       """)
        top_locations = cursor.fetchall()
        if top_locations:
            print(f"\nüìç TOP 5 ƒê·ªäA ƒêI·ªÇM C√ì NHI·ªÄU VI·ªÜC NH·∫§T:")
            for location, count in top_locations:
                print(f"   - {location}: {count} vi·ªác l√†m")

        print(f"\nüóÇÔ∏è  C·∫•u tr√∫c b·∫£ng 'jobs' hi·ªán c√≥ {len(available_columns)} c·ªôt:")
        print(f"   C√°c c·ªôt: {', '.join(available_columns[:10])}{'...' if len(available_columns) > 10 else ''}")
        print("="*50)

    except Error as e:
        print(f"‚ùå L·ªói khi l·∫•y th·ªëng k√™: {e}")
    finally:
        if connection.is_connected():
            cursor.close()
            connection.close()


def save_jobs_to_db(all_jobs_data):
    """L∆∞u t·∫•t c·∫£ d·ªØ li·ªáu c√¥ng vi·ªác v√†o database."""
    if not all_jobs_data:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u.")
        return False

    print(f"üíæ ƒêang l∆∞u {len(all_jobs_data)} c√¥ng vi·ªác v√†o database...")

    success_count = 0
    for i, job in enumerate(all_jobs_data, 1):
        # print(f"ƒêang l∆∞u job {i}/{len(all_jobs_data)}: {job.get('Ti√™u ƒë·ªÅ', 'Unknown')}") # T·∫Øt b·ªõt log cho g·ªçn
        if save_job_to_db(job):
            success_count += 1

    print(f"‚úÖ ƒê√£ l∆∞u/c·∫≠p nh·∫≠t th√†nh c√¥ng {success_count}/{len(all_jobs_data)} c√¥ng vi·ªác v√†o database")

    # Th·ªëng k√™
    connection = create_database_connection()
    if connection:
        try:
            cursor = connection.cursor()

            cursor.execute("SELECT COUNT(*) FROM jobs")
            total_jobs = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM users WHERE role = 'employer'")
            total_companies = cursor.fetchone()[0]

            print(f"\nüìà TH·ªêNG K√ä DATABASE:")
            print(f"- T·ªïng s·ªë c√¥ng vi·ªác: {total_jobs}")
            print(f"- T·ªïng s·ªë c√¥ng ty: {total_companies}")

        except Error as e:
            print(f"‚ùå L·ªói khi l·∫•y th·ªëng k√™: {e}")
        finally:
            if connection.is_connected():
                cursor.close()
                connection.close()

    return success_count > 0

def save_checkpoint_to_db(all_jobs_data, page_count):
    """L∆∞u checkpoint v√†o database."""
    print(f"üíæ ƒêang l∆∞u checkpoint t·∫°i trang {page_count}...")
    return save_jobs_to_db(all_jobs_data)


# ==============================================================================
# PH·∫¶N CODE CRAWL D·ªÆ LI·ªÜU (KH√îNG THAY ƒê·ªîI)
# ==============================================================================

def scrape_employer_details(driver, company_page_url):
    """H√†m crawl ƒë·∫ßy ƒë·ªß th√¥ng tin c√¥ng ty t·ª´ trang chi ti·∫øt c√¥ng ty."""
    company_details = {}
    additional_job_links = []

    try:
        print(f"ƒêang truy c·∫≠p trang c√¥ng ty: {company_page_url}")
        driver.get(company_page_url)
        time.sleep(2)

        # Ch·ªù trang t·∫£i xong
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, ".pt-4.m-auto.w-full.h-auto"))
        )

        # ‚≠ê CRAWL T·∫§T C·∫¢ TH√îNG TIN TRONG PH·∫¶N T·ª¨ CH√çNH
        try:
            main_container = driver.find_element(By.CSS_SELECTOR, ".pt-4.m-auto.w-full.h-auto.xl\\:w-\\[1036px\\]")
            print("‚úÖ ƒê√£ t√¨m th·∫•y container ch√≠nh c·ªßa trang c√¥ng ty")

            # Crawl t·∫•t c·∫£ text content trong container
            all_text_content = main_container.text.strip()
            if all_text_content:
                company_details['full_content'] = all_text_content
                print(f"üìã ƒê√£ crawl ƒë∆∞·ª£c {len(all_text_content)} k√Ω t·ª± n·ªôi dung t·ª´ trang c√¥ng ty")

            # Crawl c√°c th√¥ng tin c·ª• th·ªÉ
            try:
                # T√™n c√¥ng ty
                company_name_elem = main_container.find_element(By.CSS_SELECTOR,
                                                                "h1, .company-name, [class*='company-name']")
                company_details['name'] = company_name_elem.text.strip()
                print(f"üè¢ T√™n c√¥ng ty: {company_details['name']}")
            except:
                pass

            # Crawl t·∫•t c·∫£ c√°c ƒëo·∫°n vƒÉn b·∫£n
            try:
                paragraphs = main_container.find_elements(By.TAG_NAME, "p")
                paragraph_texts = []
                for p in paragraphs:
                    text = p.text.strip()
                    if text and len(text) > 10:  # Ch·ªâ l·∫•y ƒëo·∫°n vƒÉn c√≥ √Ω nghƒ©a
                        paragraph_texts.append(text)

                if paragraph_texts:
                    company_details['description'] = '\n\n'.join(paragraph_texts)
                    print(f"üìù ƒê√£ crawl ƒë∆∞·ª£c {len(paragraph_texts)} ƒëo·∫°n m√¥ t·∫£ c√¥ng ty")
            except:
                pass

            # Crawl t·∫•t c·∫£ c√°c div c√≥ th√¥ng tin
            try:
                info_divs = main_container.find_elements(By.TAG_NAME, "div")
                for div in info_divs:
                    text = div.text.strip()
                    if text and len(text) > 5:
                        # T√¨m th√¥ng tin ƒë·ªãa ch·ªâ
                        if any(keyword in text.lower() for keyword in ['ƒë·ªãa ch·ªâ', 'address', 'location', 'v·ªã tr√≠']):
                            company_details['address'] = text
                        # T√¨m th√¥ng tin website
                        elif any(keyword in text.lower() for keyword in ['website', 'web', 'trang web']):
                            company_details['website'] = text
                        # T√¨m th√¥ng tin quy m√¥
                        elif any(keyword in text.lower() for keyword in ['quy m√¥', 'nh√¢n vi√™n', 'size', 'scale']):
                            company_details['size'] = text
                        # T√¨m th√¥ng tin ng√†nh ngh·ªÅ
                        elif any(keyword in text.lower() for keyword in ['ng√†nh', 'lƒ©nh v·ª±c', 'industry', 'field']):
                            company_details['industry'] = text
            except:
                pass

            # Crawl t·∫•t c·∫£ c√°c links
            try:
                all_links = main_container.find_elements(By.TAG_NAME, "a")
                website_links = []
                for link in all_links:
                    href = link.get_attribute('href')
                    if href:
                        # L∆∞u website links
                        if any(domain in href for domain in
                               ['.com', '.vn', '.net', '.org']) and 'vieclam24h' not in href:
                            website_links.append(href)
                        # L∆∞u job links
                        elif 'tim-viec-lam' in href and href not in additional_job_links:
                            additional_job_links.append(href)

                if website_links:
                    company_details['website'] = website_links[0]  # L·∫•y website ƒë·∫ßu ti√™n
                    print(f"üåê Website c√¥ng ty: {company_details['website']}")
            except:
                pass

            # Crawl t·∫•t c·∫£ c√°c h√¨nh ·∫£nh
            try:
                images = main_container.find_elements(By.TAG_NAME, "img")
                image_urls = []
                for img in images:
                    src = img.get_attribute('src')
                    if src and 'logo' in src.lower():
                        image_urls.append(src)

                if image_urls:
                    company_details['logo_url'] = image_urls[0]
                    print(f"üñºÔ∏è Logo c√¥ng ty: {company_details['logo_url']}")
            except:
                pass

            # Crawl t·∫•t c·∫£ c√°c span v√† strong tags c√≥ th√¥ng tin quan tr·ªçng
            try:
                important_tags = main_container.find_elements(By.CSS_SELECTOR, "span, strong, b, em")
                for tag in important_tags:
                    text = tag.text.strip()
                    if text and len(text) > 3:
                        # L∆∞u th√¥ng tin c√≥ v·∫ª quan tr·ªçng
                        if any(keyword in text.lower() for keyword in ['th√†nh l·∫≠p', 'founded', 'since']):
                            company_details['founded'] = text
                        elif any(keyword in text.lower() for keyword in ['email', 'mail']):
                            company_details['email'] = text
                        elif any(keyword in text.lower() for keyword in ['phone', 'ƒëi·ªán tho·∫°i', 'tel']):
                            company_details['phone'] = text
            except:
                pass

            print(f"‚úÖ Ho√†n th√†nh crawl th√¥ng tin c√¥ng ty. Thu ƒë∆∞·ª£c {len(company_details)} tr∆∞·ªùng th√¥ng tin")

        except Exception as e:
            print(f"‚ùå L·ªói khi crawl container ch√≠nh: {e}")
            # Fallback: th·ª≠ crawl to√†n b·ªô body n·∫øu kh√¥ng t√¨m th·∫•y container ch√≠nh
            try:
                body_content = driver.find_element(By.TAG_NAME, "body").text.strip()
                if body_content:
                    company_details['full_content'] = body_content[:5000]  # Gi·ªõi h·∫°n 5000 k√Ω t·ª±
                    print("üìã ƒê√£ crawl fallback content t·ª´ body")
            except:
                pass

        # T√¨m th√™m c√°c job links kh√°c tr√™n trang
        try:
            job_link_selectors = [
                "a[href*='tim-viec-lam']",
                "a[href*='tuyen-dung']",
                ".job-list a",
                ".job-item a"
            ]

            for selector in job_link_selectors:
                try:
                    job_elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for elem in job_elements:
                        href = elem.get_attribute('href')
                        if href and href not in additional_job_links and 'tim-viec-lam' in href:
                            additional_job_links.append(href)
                except:
                    continue

            print(f"üîó T√¨m th·∫•y {len(additional_job_links)} vi·ªác l√†m b·ªï sung t·ª´ trang c√¥ng ty")

        except Exception as e:
            print(f"L·ªói khi t√¨m job links b·ªï sung: {e}")

    except Exception as e:
        print(f"‚ùå L·ªói khi truy c·∫≠p trang c√¥ng ty {company_page_url}: {e}")

    return company_details, additional_job_links

def get_job_details(driver):
    """H√†m ƒë·ªÉ crawl t·∫•t c·∫£ chi ti·∫øt c√¥ng vi·ªác t·ª´ trang job detail hi·ªán t·∫°i - ƒê√É N√ÇNG C·∫§P."""
    job_details = {}

    # ‚≠ê CRAWL T·∫§T C·∫¢ N·ªòI DUNG TRONG TH·∫∫ "wp-container pt-0"
    try:
        # print("üîç ƒêang t√¨m v√† crawl n·ªôi dung t·ª´ th·∫ª 'wp-container pt-0'...") # T·∫Øt b·ªõt log cho g·ªçn

        # Th·ª≠ c√°c selector kh√°c nhau ƒë·ªÉ t√¨m th·∫ª wp-container pt-0
        wp_container_selectors = [
            ".wp-container.pt-0",
            "[class*='wp-container'][class*='pt-0']",
            ".wp-container[class*='pt-0']",
            "div.wp-container.pt-0"
        ]

        wp_container = None
        for selector in wp_container_selectors:
            try:
                wp_container = driver.find_element(By.CSS_SELECTOR, selector)
                # print(f"‚úÖ ƒê√£ t√¨m th·∫•y wp-container pt-0 v·ªõi selector: {selector}")
                break
            except NoSuchElementException:
                continue

        if wp_container:
            # Crawl to√†n b·ªô text content t·ª´ wp-container
            full_wp_content = wp_container.text.strip()
            if full_wp_content:
                job_details["To√†n b·ªô n·ªôi dung WP"] = full_wp_content
                # print(f"üìã ƒê√£ crawl ƒë∆∞·ª£c {len(full_wp_content)} k√Ω t·ª± t·ª´ wp-container pt-0")

            # Crawl c√°c ph·∫ßn t·ª≠ con trong wp-container ƒë·ªÉ ph√¢n lo·∫°i th√¥ng tin
            try:
                # Crawl t·∫•t c·∫£ c√°c heading (h1, h2, h3, h4, h5, h6)
                headings = wp_container.find_elements(By.CSS_SELECTOR, "h1, h2, h3, h4, h5, h6")
                heading_texts = []
                for heading in headings:
                    text = heading.text.strip()
                    if text:
                        heading_texts.append(text)
                if heading_texts:
                    job_details["C√°c ti√™u ƒë·ªÅ"] = " | ".join(heading_texts)
            except:
                pass

            # Crawl t·∫•t c·∫£ c√°c ƒëo·∫°n vƒÉn (p tags)
            try:
                paragraphs = wp_container.find_elements(By.TAG_NAME, "p")
                paragraph_texts = []
                for p in paragraphs:
                    text = p.text.strip()
                    if text and len(text) > 5:  # Ch·ªâ l·∫•y ƒëo·∫°n vƒÉn c√≥ √Ω nghƒ©a
                        paragraph_texts.append(text)
                if paragraph_texts:
                    job_details["C√°c ƒëo·∫°n vƒÉn"] = "\n\n".join(paragraph_texts)
            except:
                pass

            # Crawl t·∫•t c·∫£ c√°c danh s√°ch (ul, ol)
            try:
                lists = wp_container.find_elements(By.CSS_SELECTOR, "ul, ol")
                list_texts = []
                for lst in lists:
                    text = lst.text.strip()
                    if text and len(text) > 5:
                        list_texts.append(text)
                if list_texts:
                    job_details["C√°c danh s√°ch"] = "\n\n".join(list_texts)
            except:
                pass

            # Crawl t·∫•t c·∫£ c√°c div con
            try:
                divs = wp_container.find_elements(By.TAG_NAME, "div")
                div_texts = []
                for div in divs:
                    text = div.text.strip()
                    if text and len(text) > 10:  # Ch·ªâ l·∫•y div c√≥ n·ªôi dung ƒë√°ng k·ªÉ
                        # Tr√°nh l·∫•y l·∫°i n·ªôi dung ƒë√£ c√≥ trong to√†n b·ªô container
                        if text != full_wp_content and text not in div_texts:
                            div_texts.append(text)
                if div_texts:
                    job_details["N·ªôi dung c√°c div"] = "\n\n".join(div_texts[:10])  # Gi·ªõi h·∫°n 10 div ƒë·∫ßu ti√™n
            except:
                pass

            # Crawl t·∫•t c·∫£ c√°c span v√† strong tags quan tr·ªçng
            try:
                important_elements = wp_container.find_elements(By.CSS_SELECTOR, "span, strong, b, em")
                important_texts = []
                for elem in important_elements:
                    text = elem.text.strip()
                    if text and len(text) > 3 and text not in important_texts:
                        important_texts.append(text)
                if important_texts:
                    job_details["Th√¥ng tin quan tr·ªçng"] = " | ".join(important_texts[:20])  # Gi·ªõi h·∫°n 20 ph·∫ßn t·ª≠
            except:
                pass

            # Crawl t·∫•t c·∫£ c√°c link
            try:
                links = wp_container.find_elements(By.TAG_NAME, "a")
                link_data = []
                for link in links:
                    href = link.get_attribute('href')
                    text = link.text.strip()
                    if href and text:
                        link_data.append(f"{text}: {href}")
                if link_data:
                    job_details["C√°c li√™n k·∫øt"] = " | ".join(link_data)
            except:
                pass

        else:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th·∫ª wp-container pt-0, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p crawl c≈©...")

    except Exception as e:
        print(f"‚ùå L·ªói khi crawl wp-container pt-0: {e}")

    # Gi·ªØ nguy√™n ph∆∞∆°ng ph√°p crawl c≈© l√†m fallback
    try:
        try:
            job_details["M√¥ t·∫£ c√¥ng vi·ªác"] = driver.find_element(By.ID, "job-description-section").text.strip()
        except:
            if "M√¥ t·∫£ c√¥ng vi·ªác" not in job_details: job_details["M√¥ t·∫£ c√¥ng vi·ªác"] = "Kh√¥ng c√≥ th√¥ng tin"
        try:
            job_details["Y√™u c·∫ßu c√¥ng vi·ªác"] = driver.find_element(By.ID, "job-requirement-section").text.strip()
        except:
            if "Y√™u c·∫ßu c√¥ng vi·ªác" not in job_details: job_details["Y√™u c·∫ßu c√¥ng vi·ªác"] = "Kh√¥ng c√≥ th√¥ng tin"
        try:
            job_details["Quy·ªÅn l·ª£i"] = driver.find_element(By.ID, "job-benefit-section").text.strip()
        except:
            if "Quy·ªÅn l·ª£i" not in job_details: job_details["Quy·ªÅn l·ª£i"] = "Kh√¥ng c√≥ th√¥ng tin"

        info_elements = driver.find_elements(By.CSS_SELECTOR, ".box-general-information .item")
        for item in info_elements:
            try:
                label = item.find_element(By.TAG_NAME, 'span').text.strip()
                value = item.find_element(By.TAG_NAME, 'p').text.strip()
                if "H·∫°n n·ªôp h·ªì s∆°" in label:
                    job_details["H·∫°n n·ªôp h·ªì s∆°"] = value
                elif "C·∫•p b·∫≠c" in label:
                    job_details["C·∫•p b·∫≠c"] = value
                elif "Kinh nghi·ªám" in label:
                    job_details["Kinh nghi·ªám"] = value
                elif "S·ªë l∆∞·ª£ng tuy·ªÉn" in label:
                    job_details["S·ªë l∆∞·ª£ng tuy·ªÉn"] = value
                elif "H√¨nh th·ª©c" in label:
                    job_details["H√¨nh th·ª©c"] = value
            except:
                continue
    except Exception as e:
        print(f"L·ªói khi crawl chi ti·∫øt c√¥ng vi·ªác (ph∆∞∆°ng ph√°p c≈©): {e}")

    # print(f"‚úÖ Ho√†n th√†nh crawl chi ti·∫øt c√¥ng vi·ªác. Thu ƒë∆∞·ª£c {len(job_details)} tr∆∞·ªùng th√¥ng tin") # T·∫Øt b·ªõt log
    return job_details

def setup_edge_driver():
    """Thi·∫øt l·∫≠p Edge WebDriver v·ªõi ƒë∆∞·ªùng d·∫´n c·ª• th·ªÉ."""
    options = Options()

    # C√°c option c∆° b·∫£n cho Edge
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-plugins")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0")

    # Uncomment d√≤ng d∆∞·ªõi n·∫øu mu·ªën ch·∫°y ·∫©n (headless mode)
    options.add_argument("--headless")

    try:
        # S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n Edge driver c·ª• th·ªÉ
        edge_driver_path = "D:\\EdgeDriver\\msedgedriver.exe"

        # Ki·ªÉm tra xem file driver c√≥ t·ªìn t·∫°i kh√¥ng
        if not os.path.exists(edge_driver_path):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y Edge driver t·∫°i: {edge_driver_path}")
            print("Vui l√≤ng ki·ªÉm tra ƒë∆∞·ªùng d·∫´n ho·∫∑c t·∫£i Edge driver t·ª´: https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/")
            return None

        service = Service(edge_driver_path)
        driver = webdriver.Edge(service=service, options=options)
        driver.set_page_load_timeout(30)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        print("‚úÖ ƒê√£ kh·ªüi t·∫°o Edge WebDriver th√†nh c√¥ng!")
        return driver
    except Exception as e:
        print(f"‚ùå L·ªói khi kh·ªüi t·∫°o Edge WebDriver: {e}")
        return None

# def crawl_vieclam24h_edge(max_pages=None, save_checkpoint_every=2):
#     """H√†m ƒë·ªÉ crawl d·ªØ li·ªáu t·ª´ vieclam24h.vn v√† l∆∞u v√†o MySQL Database - Phi√™n b·∫£n Edge.
#
#     Args:
#         max_pages (int, optional): S·ªë trang t·ªëi ƒëa ƒë·ªÉ crawl. N·∫øu None th√¨ crawl h·∫øt t·∫•t c·∫£.
#         save_checkpoint_every (int): L∆∞u checkpoint sau m·ªói bao nhi√™u trang.
#     """
#     # Thi·∫øt l·∫≠p database tr∆∞·ªõc khi crawl
#     if not setup_database_tables():
#         print("‚ùå Kh√¥ng th·ªÉ thi·∫øt l·∫≠p database. D·ª´ng crawl.")
#         return
#
#     driver = setup_edge_driver()
#     if not driver:
#         return
#
#     start_url = "https://vieclam24h.vn/viec-lam-it-phan-mem-o8.html"
#     all_jobs_data = []
#     page_count = 1
#     total_pages = None  # S·∫Ω ƒë∆∞·ª£c x√°c ƒë·ªãnh t·ª± ƒë·ªông
#
#     try:
#         driver.get(start_url)
#         print(f"ƒê√£ truy c·∫≠p: {start_url}")
#
#         # T·ª± ƒë·ªông ph√°t hi·ªán t·ªïng s·ªë trang
#         try:
#             print("üîç ƒêang ph√°t hi·ªán t·ªïng s·ªë trang...")
#
#             # Ch·ªù ph·∫ßn pagination load
#             WebDriverWait(driver, 10).until(
#                 EC.presence_of_element_located((By.CSS_SELECTOR, "div.grid.grid-cols-1.gap-y-2 > a"))
#             )
#
#             # Th·ª≠ c√°c selector kh√°c nhau ƒë·ªÉ t√¨m pagination
#             pagination_selectors = [
#                 ".pagination a:last-child",
#                 ".pagination .page-link:last-child",
#                 ".page-numbers:last-child",
#                 "a[aria-label='Last']",
#                 ".pagination-container a:last-child",
#                 "[class*='pagination'] a:last-child"
#             ]
#
#             for selector in pagination_selectors:
#                 try:
#                     last_page_elements = driver.find_elements(By.CSS_SELECTOR, selector)
#                     for element in last_page_elements:
#                         text = element.text.strip()
#                         href = element.get_attribute('href')
#
#                         # Th·ª≠ extract s·ªë trang t·ª´ text
#                         if text.isdigit():
#                             total_pages = int(text)
#                             print(f"‚úÖ T√¨m th·∫•y t·ªïng s·ªë trang t·ª´ text: {total_pages}")
#                             break
#
#                         # Th·ª≠ extract t·ª´ href
#                         if href and 'page=' in href:
#                             import re
#                             page_match = re.search(r'page=(\d+)', href)
#                             if page_match:
#                                 total_pages = int(page_match.group(1))
#                                 print(f"‚úÖ T√¨m th·∫•y t·ªïng s·ªë trang t·ª´ href: {total_pages}")
#                                 break
#
#                     if total_pages:
#                         break
#                 except:
#                     continue
#
#             # N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c b·∫±ng c√°ch tr√™n, th·ª≠ t√¨m t·∫•t c·∫£ s·ªë trang
#             if not total_pages:
#                 try:
#                     page_numbers = driver.find_elements(By.CSS_SELECTOR, ".pagination a, .page-numbers, [class*='page-']")
#                     max_page = 0
#                     for element in page_numbers:
#                         text = element.text.strip()
#                         if text.isdigit():
#                             max_page = max(max_page, int(text))
#
#                     if max_page > 0:
#                         total_pages = max_page
#                         print(f"‚úÖ T√¨m th·∫•y t·ªïng s·ªë trang t·ª´ danh s√°ch: {total_pages}")
#                 except:
#                     pass
#
#             # N·∫øu v·∫´n kh√¥ng t√¨m ƒë∆∞·ª£c, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p th·ª≠ nghi·ªám
#             if not total_pages:
#                 print("‚ö†Ô∏è Kh√¥ng t√¨m ƒë∆∞·ª£c t·ªïng s·ªë trang, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p t·ª± ƒë·ªông ph√°t hi·ªán...")
#                 total_pages = float('inf')  # S·∫Ω crawl cho ƒë·∫øn khi h·∫øt
#
#         except Exception as e:
#             print(f"‚ö†Ô∏è L·ªói khi ph√°t hi·ªán t·ªïng s·ªë trang: {e}")
#             total_pages = float('inf')
#
#         # Hi·ªÉn th·ªã th√¥ng tin crawl
#         if max_pages:
#             actual_max = min(max_pages, total_pages) if total_pages != float('inf') else max_pages
#             print(f"üìã S·∫º CRAWL: {actual_max} trang (gi·ªõi h·∫°n b·ªüi max_pages)")
#         else:
#             if total_pages == float('inf'):
#                 print(f"üìã S·∫º CRAWL: T·∫§T C·∫¢ c√°c trang (t·ª± ƒë·ªông d·ª´ng khi h·∫øt)")
#             else:
#                 print(f"üìã S·∫º CRAWL: T·∫§T C·∫¢ {total_pages} trang")
#
#         while True:
#             # Ki·ªÉm tra ƒëi·ªÅu ki·ªán d·ª´ng
#             if max_pages and page_count > max_pages:
#                 print(f"üõë ƒê√£ ƒë·∫°t gi·ªõi h·∫°n max_pages ({max_pages}). D·ª´ng crawl.")
#                 break
#
#             if total_pages != float('inf') and page_count > total_pages:
#                 print(f"üõë ƒê√£ crawl h·∫øt t·∫•t c·∫£ {total_pages} trang. Ho√†n th√†nh!")
#                 break
#
#             print(f"\n{'='*60}")
#             if total_pages == float('inf'):
#                 print(f"--- ƒêang crawl d·ªØ li·ªáu t·ª´ trang s·ªë: {page_count} ---")
#             else:
#                 print(f"--- ƒêang crawl d·ªØ li·ªáu t·ª´ trang s·ªë: {page_count}/{total_pages} ---")
#             print(f"{'='*60}")
#
#             try:
#                 WebDriverWait(driver, 20).until(
#                     EC.presence_of_element_located((By.CSS_SELECTOR, "div.grid.grid-cols-1.gap-y-2 > a")))
#                 job_cards = driver.find_elements(By.CSS_SELECTOR, "div.grid.grid-cols-1.gap-y-2 > a")
#                 print(f"T√¨m th·∫•y {len(job_cards)} vi·ªác l√†m tr√™n trang n√†y.")
#
#                 job_links = [card.get_attribute('href') for card in job_cards if card.get_attribute('href')]
#                 print(f"ƒê√£ l·∫•y ƒë∆∞·ª£c {len(job_links)} links ƒë·ªÉ crawl chi ti·∫øt")
#
#                 # ‚≠ê CRAWL LOGO T·ª™ TRANG DANH S√ÅCH TR∆Ø·ªöC KHI V√ÄO CHI TI·∫æT
#                 # job_logos = {}
#                 # print("üñºÔ∏è ƒêang crawl logo c√¥ng ty t·ª´ trang danh s√°ch...")
#                 # for i, job_card in enumerate(job_cards):
#                 #     try:
#                 #         # T√¨m logo trong job card v·ªõi class m·ªõi
#                 #         logo_img = job_card.find_element(By.CSS_SELECTOR, ".relative.w-full.h-full.object-contain.my-auto.rounded-md")
#                 #         logo_url = logo_img.get_attribute('src')
#                 #         if logo_url:
#                 #             job_link = job_links[i] if i < len(job_links) else None
#                 #             if job_link:
#                 #                 job_logos[job_link] = logo_url
#                 #                 # print(f"‚úÖ T√¨m th·∫•y logo cho job {i+1}: {logo_url}") # T·∫Øt b·ªõt log
#                 #     except:
#                 #         # Th·ª≠ c√°c selector kh√°c ƒë·ªÉ t√¨m logo
#                 #         try:
#                 #             logo_img = job_card.find_element(By.TAG_NAME, "img")
#                 #             logo_url = logo_img.get_attribute('src')
#                 #             if logo_url and i < len(job_links):
#                 #                 job_logos[job_links[i]] = logo_url
#                 #                 # print(f"‚úÖ T√¨m th·∫•y logo (fallback) cho job {i+1}: {logo_url}") # T·∫Øt b·ªõt log
#                 #         except:
#                 #             pass
#                 #
#                 # print(f"üìä ƒê√£ t√¨m th·∫•y logo cho {len(job_logos)} vi·ªác l√†m")
#
#                 # ‚≠ê CRAWL LOGO T·ª™ TRANG DANH S√ÅCH TR∆Ø·ªöC KHI V√ÄO CHI TI·∫æT - DEBUG & FIX
#                 job_logos = {}
#                 print("üñºÔ∏è ƒêang crawl logo c√¥ng ty t·ª´ trang danh s√°ch...")
#
#                 for i, job_card in enumerate(job_cards):
#                     try:
#                         print(f"üîç Debug job {i+1}: ƒêang t√¨m logo...")
#
#                         # DEBUG: In ra HTML structure ƒë·ªÉ xem
#                         try:
#                             card_html = job_card.get_attribute('innerHTML')
#                             if i == 0:  # Ch·ªâ debug job ƒë·∫ßu ti√™n
#                                 print(f"üìã HTML structure c·ªßa job ƒë·∫ßu ti√™n:")
#                                 # T√¨m t·∫•t c·∫£ img tags
#                                 import re
#                                 img_matches = re.findall(r'<img[^>]*>', card_html)
#                                 for idx, img_tag in enumerate(img_matches):
#                                     print(f"   IMG {idx+1}: {img_tag[:100]}...")
#                         except:
#                             pass
#
#                         logo_url = None
#                         job_link = job_links[i] if i < len(job_links) else None
#
#                         # Ph∆∞∆°ng ph√°p c·∫£i ti·∫øn ƒë·ªÉ t√¨m logo
#                         logo_selectors = [
#                             # Th·ª≠ v·ªõi class ch√≠nh x√°c (kh√¥ng c√≥ d·∫•u ch·∫•m gi·ªØa c√°c class)
#                             "img[class='relative w-full h-full object-contain my-auto rounded-md']",
#                             # Th·ª≠ v·ªõi partial matching cho t·ª´ng class
#                             "img[class*='relative'][class*='w-full'][class*='h-full'][class*='object-contain'][class*='my-auto'][class*='rounded-md']",
#                             # Th·ª≠ v·ªõi m·ªôt s·ªë class ch√≠nh
#                             "img[class*='object-contain'][class*='rounded-md']",
#                             # Th·ª≠ v·ªõi class relative v√† w-full
#                             "img[class*='relative'][class*='w-full']",
#                             # Th·ª≠ t√¨m img v·ªõi class ch·ª©a object-contain
#                             "img[class*='object-contain']",
#                             # Fallback: b·∫•t k·ª≥ img n√†o
#                             "img"
#                         ]
#
#                         # Th·ª≠ t·ª´ng selector
#                         for idx, selector in enumerate(logo_selectors):
#                             try:
#                                 logo_imgs = job_card.find_elements(By.CSS_SELECTOR, selector)
#
#                                 if logo_imgs:
#                                     print(f"üéØ Selector {idx+1} '{selector}' t√¨m th·∫•y {len(logo_imgs)} ·∫£nh")
#
#                                     for img in logo_imgs:
#                                         src = img.get_attribute('src')
#                                         class_attr = img.get_attribute('class')
#
#                                         if src:
#                                             # ∆Øu ti√™n ·∫£nh c√≥ class ch·ª©a c√°c t·ª´ kh√≥a quan tr·ªçng
#                                             if class_attr and any(keyword in class_attr for keyword in ['object-contain', 'rounded-md', 'relative']):
#                                                 logo_url = src
#                                                 print(f"‚úÖ T√¨m th·∫•y logo job {i+1} v·ªõi class: {class_attr}")
#                                                 print(f"   URL: {src[:60]}...")
#                                                 break
#                                             elif not logo_url:  # Backup
#                                                 logo_url = src
#                                                 print(f"üíº Backup logo job {i+1}: {src[:60]}...")
#
#                                     if logo_url:
#                                         break
#
#                             except Exception as selector_error:
#                                 if i == 0:  # Ch·ªâ log l·ªói cho job ƒë·∫ßu ti√™n
#                                     print(f"   ‚ùå Selector {idx+1} l·ªói: {selector_error}")
#                                 continue
#
#                         # Ph∆∞∆°ng ph√°p cu·ªëi c√πng: T√¨m t·∫•t c·∫£ img v√† filter
#                         if not logo_url:
#                             try:
#                                 all_imgs = job_card.find_elements(By.TAG_NAME, "img")
#                                 print(f"üîé Fallback: T√¨m th·∫•y {len(all_imgs)} ·∫£nh trong job {i+1}")
#
#                                 for img in all_imgs:
#                                     src = img.get_attribute('src')
#                                     alt = img.get_attribute('alt') or ""
#                                     class_attr = img.get_attribute('class') or ""
#
#                                     if src:
#                                         # ∆Øu ti√™n ·∫£nh c√≥ alt ho·∫∑c src ch·ª©a logo
#                                         if any(keyword in (src + alt).lower() for keyword in ['logo', 'company', 'brand']):
#                                             logo_url = src
#                                             print(f"üè¢ Logo t·ª´ alt/src keyword job {i+1}: {src[:60]}...")
#                                             break
#                                         # ∆Øu ti√™n ·∫£nh c√≥ class ph√π h·ª£p
#                                         elif any(keyword in class_attr for keyword in ['object-contain', 'rounded']):
#                                             logo_url = src
#                                             print(f"üé® Logo t·ª´ class job {i+1}: {src[:60]}...")
#                                             break
#                                         # L·∫•y ·∫£nh ƒë·∫ßu ti√™n n·∫øu ch∆∞a c√≥
#                                         elif not logo_url:
#                                             logo_url = src
#                                             print(f"üì∑ First image job {i+1}: {src[:60]}...")
#                             except:
#                                 pass
#
#                         # L∆∞u k·∫øt qu·∫£
#                         if logo_url and job_link:
#                             job_logos[job_link] = logo_url
#                             print(f"üíæ ƒê√£ l∆∞u logo job {i+1}")
#                         else:
#                             print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y logo cho job {i+1}")
#
#                         # Ch·ªâ debug chi ti·∫øt cho job ƒë·∫ßu ti√™n
#                         if i == 0:
#                             print(f"üî¨ Debug job ƒë·∫ßu ti√™n ho√†n t·∫•t\n")
#
#                     except Exception as e:
#                         print(f"‚ùå L·ªói khi crawl logo job {i+1}: {e}")
#                         continue
#
#                 print(f"üìä T·ªïng k·∫øt: ƒê√£ t√¨m th·∫•y logo cho {len(job_logos)}/{len(job_cards)} vi·ªác l√†m")
#
#
#
#                 listing_page_url = driver.current_url
#
#                 temp_jobs_this_page = []
#
#                 for i, job_link in enumerate(job_links, 1):
#                     try:
#                         print(f"\n{'*'*50}")
#                         print(f"ƒêang x·ª≠ l√Ω c√¥ng vi·ªác {i}/{len(job_links)} (Trang {page_count})")
#                         print(f"{'*'*50}")
#                         driver.get(job_link)
#
#                         # Ch·ªù cho ti√™u ƒë·ªÅ (h1) xu·∫•t hi·ªán ƒë·ªÉ ch·∫Øc ch·∫Øn trang ƒë√£ t·∫£i
#                         WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "h1")))
#
#                         job_title, company_name, salary, location, logo_url = "Kh√¥ng x√°c ƒë·ªãnh", "Kh√¥ng x√°c ƒë·ªãnh", "Tho·∫£ thu·∫≠n", "Kh√¥ng x√°c ƒë·ªãnh", None
#
#                         try:
#                             job_title = driver.find_element(By.CSS_SELECTOR, "h1").text.strip()
#                         except:
#                             pass
#                         try:
#                             company_name = driver.find_element(By.CSS_SELECTOR,
#                                                                ".box-company-info .company-name").text.strip()
#                         except:
#                             pass
#
#                         # ∆Øu ti√™n s·ª≠ d·ª•ng logo t·ª´ trang danh s√°ch ƒë√£ crawl tr∆∞·ªõc
#                         if job_link in job_logos:
#                             logo_url = job_logos[job_link]
#                         else:
#                             # N·∫øu kh√¥ng c√≥, th·ª≠ t√¨m logo tr√™n trang chi ti·∫øt
#                             try:
#                                 logo_url = driver.find_element(By.CSS_SELECTOR, ".box-company-logo img").get_attribute('src')
#                             except:
#                                 pass
#
#                         try:
#                             salary = driver.find_element(By.CSS_SELECTOR, ".box-general-information .salary").text.strip()
#                         except:
#                             pass
#                         try:
#                             location = driver.find_element(By.CSS_SELECTOR,
#                                                            ".box-general-information .address").text.strip()
#                         except:
#                             pass
#
#                         job_details = get_job_details(driver)
#
#                         # Crawl th√¥ng tin c√¥ng ty
#                         company_details = {}
#                         try:
#                             # print("ƒêang t√¨m link ƒë·∫øn trang chi ti·∫øt c√¥ng ty...") # T·∫Øt b·ªõt log
#                             wait = WebDriverWait(driver, 5)
#
#                             company_link_selectors = [
#                                 ".box-company-info a[href*='danh-sach-tin-tuyen-dung-cong-ty-']",
#                                 "a.company-name[href*='danh-sach-tin-tuyen-dung-cong-ty-']",
#                                 "a[href*='danh-sach-tin-tuyen-dung-cong-ty-']"
#                             ]
#
#                             company_page_url = None
#                             for selector in company_link_selectors:
#                                 try:
#                                     company_page_link_element = wait.until(
#                                         EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
#                                     company_page_url = company_page_link_element.get_attribute('href')
#                                     if company_page_url:
#                                         print("‚úÖ ƒê√£ t√¨m th·∫•y link c√¥ng ty.")
#                                         break
#                                 except TimeoutException:
#                                     continue
#
#                             if company_page_url:
#                                 company_details, additional_job_links = scrape_employer_details(driver, company_page_url)
#                                 print("ƒê√£ crawl xong trang c√¥ng ty, quay l·∫°i trang tin tuy·ªÉn d·ª•ng.")
#
#                                 # X·ª≠ l√Ω c√°c vi·ªác l√†m b·ªï sung (gi·ªõi h·∫°n ƒë·ªÉ tr√°nh qu√° t·∫£i)
#                                 if additional_job_links and len(additional_job_links) <= 3:  # Gi·ªõi h·∫°n ch·ªâ 3 vi·ªác b·ªï sung
#                                     print(f"T√¨m th·∫•y {len(additional_job_links)} vi·ªác l√†m b·ªï sung t·ª´ trang c√¥ng ty.")
#                                     current_job_url = driver.current_url
#
#                                     for add_job_link in additional_job_links:
#                                         if add_job_link not in [job['Link'] for job in temp_jobs_this_page] and add_job_link not in job_links:
#                                             try:
#                                                 print(f"ƒêang crawl vi·ªác l√†m b·ªï sung: {add_job_link}")
#                                                 driver.get(add_job_link)
#                                                 WebDriverWait(driver, 10).until(
#                                                     EC.presence_of_element_located((By.TAG_NAME, "h1")))
#
#                                                 # Crawl th√¥ng tin c∆° b·∫£n c·ªßa vi·ªác l√†m b·ªï sung
#                                                 add_job_title = "Kh√¥ng x√°c ƒë·ªãnh"
#                                                 add_company_name = company_name
#                                                 add_salary = "Tho·∫£ thu·∫≠n"
#                                                 add_location = "Kh√¥ng x√°c ƒë·ªãnh"
#                                                 add_logo_url = logo_url
#
#                                                 try:
#                                                     add_job_title = driver.find_element(By.CSS_SELECTOR, "h1").text.strip()
#                                                 except:
#                                                     pass
#                                                 try:
#                                                     add_salary = driver.find_element(By.CSS_SELECTOR, ".box-general-information .salary").text.strip()
#                                                 except:
#                                                     pass
#                                                 try:
#                                                     add_location = driver.find_element(By.CSS_SELECTOR, ".box-general-information .address").text.strip()
#                                                 except:
#                                                     pass
#
#                                                 add_job_details = get_job_details(driver)
#
#                                                 # T·∫°o dict cho vi·ªác l√†m b·ªï sung
#                                                 additional_job_data = {
#                                                     "Ti√™u ƒë·ªÅ": add_job_title,
#                                                     "C√¥ng ty": add_company_name,
#                                                     "Logo URL": add_logo_url,
#                                                     "M·ª©c l∆∞∆°ng": add_salary,
#                                                     "ƒê·ªãa ƒëi·ªÉm": add_location,
#                                                     "Link": add_job_link,
#                                                     "Trang": f"{page_count} (B·ªï sung)",
#                                                     "Company Details": company_details,
#                                                     **add_job_details
#                                                 }
#
#                                                 temp_jobs_this_page.append(additional_job_data)
#                                                 print(f"‚úÖ ƒê√£ th√™m vi·ªác l√†m b·ªï sung: {add_job_title}")
#
#                                             except Exception as e:
#                                                 print(f"‚ùå L·ªói khi crawl vi·ªác l√†m b·ªï sung {add_job_link}: {e}")
#                                                 continue
#
#                                     # Quay l·∫°i trang vi·ªác l√†m ch√≠nh
#                                     driver.get(current_job_url)
#                                     WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "h1")))
#
#                             else:
#                                 print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y link trang c√¥ng ty.")
#
#                         except Exception as company_error:
#                             print(f"‚ùå L·ªói khi crawl th√¥ng tin c√¥ng ty: {company_error}")
#
#                         # T·∫°o dict ch·ª©a t·∫•t c·∫£ th√¥ng tin c·ªßa c√¥ng vi·ªác ch√≠nh
#                         job_data = {
#                             "Ti√™u ƒë·ªÅ": job_title,
#                             "C√¥ng ty": company_name,
#                             "Logo URL": logo_url,
#                             "M·ª©c l∆∞∆°ng": salary,
#                             "ƒê·ªãa ƒëi·ªÉm": location,
#                             "Link": job_link,
#                             "Trang": page_count,
#                             "Company Details": company_details,
#                             **job_details
#                         }
#
#                         temp_jobs_this_page.append(job_data)
#                         print(f"‚úÖ ƒê√£ crawl xong c√¥ng vi·ªác: {job_title}")
#                         print(f"üìä T·ªïng c·ªông ƒë√£ c√≥ {len(all_jobs_data) + len(temp_jobs_this_page)} vi·ªác l√†m")
#
#                         # Ngh·ªâ gi·ªØa c√°c request
#                         time.sleep(2)
#
#                     except Exception as job_error:
#                         print(f"‚ùå L·ªói khi x·ª≠ l√Ω c√¥ng vi·ªác {job_link}: {job_error}")
#                         # Quay l·∫°i trang danh s√°ch ƒë·ªÉ ti·∫øp t·ª•c
#                         driver.get(listing_page_url)
#                         continue
#
#                 all_jobs_data.extend(temp_jobs_this_page)
#
#                 # L∆∞u checkpoint sau m·ªói v√†i trang
#                 if page_count % save_checkpoint_every == 0 and temp_jobs_this_page:
#                     save_checkpoint_to_db(temp_jobs_this_page, page_count)
#                     all_jobs_data = [] # X√≥a checkpoint ƒë√£ l∆∞u ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ
#
#                 # Chuy·ªÉn sang trang ti·∫øp theo
#                 try:
#                     # Quay v·ªÅ trang danh s√°ch
#                     if driver.current_url != listing_page_url:
#                         driver.get(listing_page_url)
#                     WebDriverWait(driver, 20).until(
#                         EC.presence_of_element_located((By.CSS_SELECTOR, "div.grid.grid-cols-1.gap-y-2 > a")))
#
#                     # T√¨m n√∫t "Trang ti·∫øp theo" v·ªõi nhi·ªÅu ph∆∞∆°ng ph√°p
#                     next_page_found = False
#
#                     # Ph∆∞∆°ng ph√°p 1: T√¨m n√∫t Next/Ti·∫øp theo
#                     next_page_selectors = [
#                         "a[aria-label='Next']",
#                         "a[aria-label='next']",
#                         ".pagination a.next",
#                         ".pagination .next",
#                         "a.next-page",
#                         ".page-item.next a",
#                         ".pagination-next a"
#                     ]
#
#                     for selector in next_page_selectors:
#                         try:
#                             next_button = driver.find_element(By.CSS_SELECTOR, selector)
#                             if next_button and next_button.is_enabled() and next_button.is_displayed():
#                                 driver.execute_script("arguments[0].click();", next_button)
#                                 time.sleep(3)
#                                 next_page_found = True
#                                 print(f"‚úÖ ƒê√£ chuy·ªÉn sang trang {page_count + 1} (method: next button)")
#                                 break
#                         except:
#                             continue
#
#                     # Ph∆∞∆°ng ph√°p 2: T√¨m text "Ti·∫øp" ho·∫∑c "Next"
#                     if not next_page_found:
#                         try:
#                             xpath_selectors = [
#                                 "//a[contains(text(), 'Ti·∫øp') or contains(text(), 'ti·∫øp')]",
#                                 "//a[contains(text(), 'Next') or contains(text(), 'next')]",
#                                 "//a[contains(text(), '‚Üí') or contains(text(), '>')]"
#                             ]
#
#                             for xpath in xpath_selectors:
#                                 try:
#                                     next_button = driver.find_element(By.XPATH, xpath)
#                                     if next_button and next_button.is_enabled() and next_button.is_displayed():
#                                         driver.execute_script("arguments[0].click();", next_button)
#                                         time.sleep(3)
#                                         next_page_found = True
#                                         print(f"‚úÖ ƒê√£ chuy·ªÉn sang trang {page_count + 1} (method: text search)")
#                                         break
#                                 except:
#                                     continue
#
#                                 if next_page_found:
#                                     break
#                         except:
#                             pass
#
#                     # Ph∆∞∆°ng ph√°p 3: T√¨m link trang ti·∫øp theo b·∫±ng s·ªë
#                     if not next_page_found:
#                         try:
#                             next_page_num = page_count + 1
#                             page_link_selectors = [
#                                 f"a[href*='page={next_page_num}']",
#                                 f".pagination a:contains('{next_page_num}')",
#                                 f"a.page-link[href*='{next_page_num}']"
#                             ]
#
#                             for selector in page_link_selectors:
#                                 try:
#                                     if ":contains" in selector:
#                                         # S·ª≠ d·ª•ng XPath cho contains text
#                                         next_button = driver.find_element(By.XPATH, f"//a[contains(text(), '{next_page_num}')]")
#                                     else:
#                                         next_button = driver.find_element(By.CSS_SELECTOR, selector)
#
#                                     if next_button and next_button.is_enabled() and next_button.is_displayed():
#                                         driver.execute_script("arguments[0].click();", next_button)
#                                         time.sleep(3)
#                                         next_page_found = True
#                                         print(f"‚úÖ ƒê√£ chuy·ªÉn sang trang {page_count + 1} (method: page number)")
#                                         break
#                                 except:
#                                     continue
#
#                                 if next_page_found:
#                                     break
#                         except:
#                             pass
#
#                     # Ph∆∞∆°ng ph√°p 4: Thay ƒë·ªïi URL tr·ª±c ti·∫øp
#                     if not next_page_found:
#                         try:
#                             current_url = driver.current_url
#                             next_page_num = page_count + 1
#
#                             # Th·ª≠ c√°c pattern URL kh√°c nhau
#                             url_patterns = [
#                                 (f"page={page_count}", f"page={next_page_num}"),
#                                 (f"page-{page_count}", f"page-{next_page_num}"),
#                                 (f"/p{page_count}", f"/p{next_page_num}"),
#                                 (f"-p{page_count}", f"-p{next_page_num}")
#                             ]
#
#                             next_url = None
#                             for old_pattern, new_pattern in url_patterns:
#                                 if old_pattern in current_url:
#                                     next_url = current_url.replace(old_pattern, new_pattern)
#                                     break
#
#                             # N·∫øu kh√¥ng t√¨m th·∫•y pattern, th√™m parameter page
#                             if not next_url:
#                                 if "page=" not in current_url:
#                                     separator = "&" if "?" in current_url else "?"
#                                     next_url = f"{current_url}{separator}page={next_page_num}"
#
#                             if next_url:
#                                 print(f"üîÑ Th·ª≠ chuy·ªÉn trang b·∫±ng URL: {next_url}")
#                                 driver.get(next_url)
#                                 time.sleep(3)
#
#                                 # Ki·ªÉm tra xem c√≥ vi·ªác l√†m tr√™n trang m·ªõi kh√¥ng
#                                 try:
#                                     new_job_cards = WebDriverWait(driver, 10).until(
#                                         EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.grid.grid-cols-1.gap-y-2 > a"))
#                                     )
#                                     if new_job_cards and len(new_job_cards) > 0:
#                                         next_page_found = True
#                                         print(f"‚úÖ ƒê√£ chuy·ªÉn sang trang {page_count + 1} (method: URL manipulation)")
#                                     else:
#                                         print("‚ö†Ô∏è Trang ti·∫øp theo kh√¥ng c√≥ vi·ªác l√†m n√†o - c√≥ th·ªÉ ƒë√£ h·∫øt trang")
#                                 except:
#                                     print("‚ö†Ô∏è Kh√¥ng th·ªÉ load trang ti·∫øp theo - c√≥ th·ªÉ ƒë√£ h·∫øt trang")
#                         except Exception as url_error:
#                             print(f"‚ùå L·ªói khi thay ƒë·ªïi URL: {url_error}")
#
#                     # Ph∆∞∆°ng ph√°p 5: Ki·ªÉm tra cu·ªëi c√πng b·∫±ng c√°ch so s√°nh s·ªë vi·ªác l√†m
#                     if not next_page_found:
#                         try:
#                             # Th·ª≠ load trang ti·∫øp theo b·∫±ng c√°ch th√™m page parameter
#                             base_url = start_url.split('?')[0]  # L·∫•y base URL
#                             next_page_num = page_count + 1
#                             test_url = f"{base_url}?page={next_page_num}"
#
#                             print(f"üîÑ Ki·ªÉm tra cu·ªëi c√πng v·ªõi URL: {test_url}")
#                             driver.get(test_url)
#                             time.sleep(3)
#
#                             # Ki·ªÉm tra c√≥ vi·ªác l√†m kh√¥ng
#                             test_job_cards = driver.find_elements(By.CSS_SELECTOR, "div.grid.grid-cols-1.gap-y-2 > a")
#                             if test_job_cards and len(test_job_cards) > 0:
#                                 next_page_found = True
#                                 print(f"‚úÖ ƒê√£ chuy·ªÉn sang trang {page_count + 1} (method: base URL + page param)")
#                             else:
#                                 print("üèÅ Kh√¥ng c√≤n trang n√†o ƒë·ªÉ crawl - ƒê√£ h·∫øt d·ªØ li·ªáu!")
#                         except:
#                             print("üèÅ Kh√¥ng th·ªÉ truy c·∫≠p trang ti·∫øp theo - K·∫øt th√∫c crawl")
#
#                     if not next_page_found:
#                         print("üèÅ ƒê√É CRAWL HET T·∫§T C·∫¢ C√ÅC TRANG C√ì TH·ªÇ!")
#                         print(f"üìä T·ªïng c·ªông ƒë√£ crawl {page_count} trang.")
#                         break
#
#                 except Exception as pagination_error:
#                     print(f"‚ùå L·ªói khi chuy·ªÉn trang: {pagination_error}")
#                     print("üèÅ K·∫øt th√∫c crawl do l·ªói ph√¢n trang")
#                     break
#
#                 page_count += 1
#
#             except Exception as page_error:
#                 print(f"‚ùå L·ªói khi x·ª≠ l√Ω trang {page_count}: {page_error}")
#                 break
#
#         # L∆∞u t·∫•t c·∫£ d·ªØ li·ªáu c√≤n l·∫°i v√†o database
#         print(f"\nüéâ HO√ÄN TH√ÄNH CRAWL D·ªÆ LI·ªÜU!")
#         if all_jobs_data:
#             print(f"üíæ ƒêang l∆∞u {len(all_jobs_data)} c√¥ng vi·ªác cu·ªëi c√πng v√†o database...")
#             save_jobs_to_db(all_jobs_data)
#
#     except Exception as main_error:
#         print(f"‚ùå L·ªói ch√≠nh trong qu√° tr√¨nh crawl: {main_error}")
#
#         # V·∫´n c·ªë g·∫Øng l∆∞u d·ªØ li·ªáu ƒë√£ crawl ƒë∆∞·ª£c v√†o database
#         if all_jobs_data:
#             print(f"üíæ ƒêang l∆∞u d·ªØ li·ªáu kh·∫©n c·∫•p ({len(all_jobs_data)} jobs) v√†o database...")
#             save_jobs_to_db(all_jobs_data)
#
#     finally:
#         try:
#             driver.quit()
#             print("‚úÖ ƒê√£ ƒë√≥ng tr√¨nh duy·ªát")
#         except:
#             pass

def crawl_vieclam24h_edge(max_pages=None, save_checkpoint_every=2):
    """H√†m ƒë·ªÉ crawl d·ªØ li·ªáu t·ª´ vieclam24h.vn v√† l∆∞u v√†o MySQL Database - Phi√™n b·∫£n Edge.

    Args:
        max_pages (int, optional): S·ªë trang t·ªëi ƒëa ƒë·ªÉ crawl. N·∫øu None th√¨ crawl h·∫øt t·∫•t c·∫£.
        save_checkpoint_every (int): L∆∞u checkpoint sau m·ªói bao nhi√™u trang.
    """
    # Thi·∫øt l·∫≠p database tr∆∞·ªõc khi crawl
    if not setup_database_tables():
        print("‚ùå Kh√¥ng th·ªÉ thi·∫øt l·∫≠p database. D·ª´ng crawl.")
        return

    driver = setup_edge_driver()
    if not driver:
        return

    start_url = "https://vieclam24h.vn/viec-lam-marketing-o12.html"
    all_jobs_data = []
    page_count = 1
    total_pages = None  # S·∫Ω ƒë∆∞·ª£c x√°c ƒë·ªãnh t·ª± ƒë·ªông

    try:
        driver.get(start_url)
        print(f"ƒê√£ truy c·∫≠p: {start_url}")

        # T·ª± ƒë·ªông ph√°t hi·ªán t·ªïng s·ªë trang
        try:
            print("üîç ƒêang ph√°t hi·ªán t·ªïng s·ªë trang...")

            # Ch·ªù ph·∫ßn pagination load
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.grid.grid-cols-1.gap-y-2 > a"))
            )

            # Th·ª≠ c√°c selector kh√°c nhau ƒë·ªÉ t√¨m pagination
            pagination_selectors = [
                ".pagination a:last-child",
                ".pagination .page-link:last-child",
                ".page-numbers:last-child",
                "a[aria-label='Last']",
                ".pagination-container a:last-child",
                "[class*='pagination'] a:last-child"
            ]

            for selector in pagination_selectors:
                try:
                    last_page_elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in last_page_elements:
                        text = element.text.strip()
                        href = element.get_attribute('href')

                        # Th·ª≠ extract s·ªë trang t·ª´ text
                        if text.isdigit():
                            total_pages = int(text)
                            print(f"‚úÖ T√¨m th·∫•y t·ªïng s·ªë trang t·ª´ text: {total_pages}")
                            break

                        # Th·ª≠ extract t·ª´ href
                        if href and 'page=' in href:
                            import re
                            page_match = re.search(r'page=(\d+)', href)
                            if page_match:
                                total_pages = int(page_match.group(1))
                                print(f"‚úÖ T√¨m th·∫•y t·ªïng s·ªë trang t·ª´ href: {total_pages}")
                                break

                    if total_pages:
                        break
                except:
                    continue

            # N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c b·∫±ng c√°ch tr√™n, th·ª≠ t√¨m t·∫•t c·∫£ s·ªë trang
            if not total_pages:
                try:
                    page_numbers = driver.find_elements(By.CSS_SELECTOR, ".pagination a, .page-numbers, [class*='page-']")
                    max_page = 0
                    for element in page_numbers:
                        text = element.text.strip()
                        if text.isdigit():
                            max_page = max(max_page, int(text))

                    if max_page > 0:
                        total_pages = max_page
                        print(f"‚úÖ T√¨m th·∫•y t·ªïng s·ªë trang t·ª´ danh s√°ch: {total_pages}")
                except:
                    pass

            # N·∫øu v·∫´n kh√¥ng t√¨m ƒë∆∞·ª£c, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p th·ª≠ nghi·ªám
            if not total_pages:
                print("‚ö†Ô∏è Kh√¥ng t√¨m ƒë∆∞·ª£c t·ªïng s·ªë trang, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p t·ª± ƒë·ªông ph√°t hi·ªán...")
                total_pages = float('inf')  # S·∫Ω crawl cho ƒë·∫øn khi h·∫øt

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ph√°t hi·ªán t·ªïng s·ªë trang: {e}")
            total_pages = float('inf')

        # Hi·ªÉn th·ªã th√¥ng tin crawl
        if max_pages:
            actual_max = min(max_pages, total_pages) if total_pages != float('inf') else max_pages
            print(f"üìã S·∫º CRAWL: {actual_max} trang (gi·ªõi h·∫°n b·ªüi max_pages)")
        else:
            if total_pages == float('inf'):
                print(f"üìã S·∫º CRAWL: T·∫§T C·∫¢ c√°c trang (t·ª± ƒë·ªông d·ª´ng khi h·∫øt)")
            else:
                print(f"üìã S·∫º CRAWL: T·∫§T C·∫¢ {total_pages} trang")

        while True:
            # Ki·ªÉm tra ƒëi·ªÅu ki·ªán d·ª´ng
            if max_pages and page_count > max_pages:
                print(f"üõë ƒê√£ ƒë·∫°t gi·ªõi h·∫°n max_pages ({max_pages}). D·ª´ng crawl.")
                break

            if total_pages != float('inf') and page_count > total_pages:
                print(f"üõë ƒê√£ crawl h·∫øt t·∫•t c·∫£ {total_pages} trang. Ho√†n th√†nh!")
                break

            print(f"\n{'='*60}")
            if total_pages == float('inf'):
                print(f"--- ƒêang crawl d·ªØ li·ªáu t·ª´ trang s·ªë: {page_count} ---")
            else:
                print(f"--- ƒêang crawl d·ªØ li·ªáu t·ª´ trang s·ªë: {page_count}/{total_pages} ---")
            print(f"{'='*60}")

            try:
                WebDriverWait(driver, 20).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "div.grid.grid-cols-1.gap-y-2 > a")))
                job_cards = driver.find_elements(By.CSS_SELECTOR, "div.grid.grid-cols-1.gap-y-2 > a")
                print(f"T√¨m th·∫•y {len(job_cards)} vi·ªác l√†m tr√™n trang n√†y.")

                job_links = [card.get_attribute('href') for card in job_cards if card.get_attribute('href')]
                print(f"ƒê√£ l·∫•y ƒë∆∞·ª£c {len(job_links)} links ƒë·ªÉ crawl chi ti·∫øt")

                # ‚≠ê CRAWL LOGO T·ª™ TRANG DANH S√ÅCH TR∆Ø·ªöC KHI V√ÄO CHI TI·∫æT - IMPROVED VERSION
                job_logos = {}
                print("üñºÔ∏è ƒêang crawl logo c√¥ng ty t·ª´ trang danh s√°ch...")

                # Scroll trang ƒë·ªÉ trigger lazy loading cho t·∫•t c·∫£ ·∫£nh
                print("üìú ƒêang scroll ƒë·ªÉ load t·∫•t c·∫£ ·∫£nh...")
                try:
                    # Scroll t·ª´ng ph·∫ßn nh·ªè ƒë·ªÉ trigger lazy loading
                    total_height = driver.execute_script("return document.body.scrollHeight")
                    current_position = 0
                    scroll_step = total_height // 10  # Chia th√†nh 10 ph·∫ßn

                    for i in range(11):  # Scroll 11 l·∫ßn ƒë·ªÉ ƒë·∫£m b·∫£o
                        driver.execute_script(f"window.scrollTo(0, {current_position})")
                        time.sleep(0.5)  # ƒê·ª£i ·∫£nh load
                        current_position += scroll_step

                    # Scroll v·ªÅ ƒë·∫ßu trang
                    driver.execute_script("window.scrollTo(0, 0)")
                    time.sleep(1)

                    print("‚úÖ Ho√†n th√†nh scroll ƒë·ªÉ load ·∫£nh")
                except Exception as scroll_error:
                    print(f"‚ö†Ô∏è L·ªói khi scroll: {scroll_error}")

                # Force trigger lazy loading images b·∫±ng JavaScript
                try:
                    print("üîÑ ƒêang force load t·∫•t c·∫£ lazy images...")
                    driver.execute_script("""
                                          // T√¨m t·∫•t c·∫£ ·∫£nh c√≥ data-src (lazy loading)
                                          const lazyImages = document.querySelectorAll('img[data-src], img[loading="lazy"]');
                                          lazyImages.forEach(img => {
                                              if (img.dataset.src) {
                                                  img.src = img.dataset.src;
                                              }
                                              // Trigger load event
                                              img.scrollIntoView({behavior: 'instant', block: 'center'});
                                          });

                                          // Force load images v·ªõi Intersection Observer n·∫øu c√≥
                                          if (window.IntersectionObserver) {
                                              const observer = new IntersectionObserver((entries) => {
                                                  entries.forEach(entry => {
                                                      if (entry.target.dataset.src) {
                                                          entry.target.src = entry.target.dataset.src;
                                                      }
                                                  });
                                              });

                                              document.querySelectorAll('img[data-src]').forEach(img => {
                                                  observer.observe(img);
                                              });
                                          }
                                          """)
                    time.sleep(2)  # ƒê·ª£i images load
                    print("‚úÖ ƒê√£ force load lazy images")
                except Exception as js_error:
                    print(f"‚ö†Ô∏è L·ªói khi force load images: {js_error}")

                for i, job_card in enumerate(job_cards):
                    try:
                        print(f"üîç Crawl logo job {i+1}/{len(job_cards)}...")

                        logo_url = None
                        job_link = job_links[i] if i < len(job_links) else None

                        # Scroll ƒë·∫øn job card c·ª• th·ªÉ ƒë·ªÉ ƒë·∫£m b·∫£o ·∫£nh ƒë∆∞·ª£c load
                        try:
                            driver.execute_script("arguments[0].scrollIntoView({behavior: 'instant', block: 'center'});", job_card)
                            time.sleep(0.3)
                        except:
                            pass

                        # Ph∆∞∆°ng ph√°p c·∫£i ti·∫øn ƒë·ªÉ t√¨m logo
                        logo_selectors = [
                            # Selector ch√≠nh x√°c nh·∫•t
                            "img.relative.w-full.h-full.object-contain.my-auto.rounded-md",
                            # Selector v·ªõi attribute matching
                            "img[class*='relative'][class*='w-full'][class*='h-full'][class*='object-contain']",
                            # T√¨m theo class ch·ª©a t·ª´ kh√≥a quan tr·ªçng
                            "img[class*='object-contain'][class*='rounded']",
                            "img[class*='company-logo']",
                            "img[class*='logo']",
                            # Fallback selector
                            "img[class*='relative'][class*='w-full']",
                            "img[class*='object-contain']",
                            "img[alt*='logo' i]",
                            "img[alt*='company' i]",
                            "img"
                        ]

                        # Th·ª≠ t·ª´ng selector v·ªõi retry mechanism
                        for idx, selector in enumerate(logo_selectors):
                            try:
                                logo_imgs = job_card.find_elements(By.CSS_SELECTOR, selector)

                                if logo_imgs:
                                    for img in logo_imgs:
                                        # ƒê·∫£m b·∫£o ·∫£nh ƒë∆∞·ª£c load
                                        try:
                                            driver.execute_script("arguments[0].scrollIntoView({behavior: 'instant', block: 'center'});", img)
                                            time.sleep(0.2)
                                        except:
                                            pass

                                        # L·∫•y src t·ª´ nhi·ªÅu attribute kh√°c nhau
                                        src_candidates = [
                                            img.get_attribute('src'),
                                            img.get_attribute('data-src'),
                                            img.get_attribute('data-original'),
                                            img.get_attribute('data-lazy-src'),
                                            img.get_attribute('data-srcset')
                                        ]

                                        for src in src_candidates:
                                            if src and src.strip() and not src.startswith('data:'):
                                                # Ki·ªÉm tra URL h·ª£p l·ªá
                                                if src.startswith(('http://', 'https://', '//')):
                                                    logo_url = src.strip()

                                                    class_attr = img.get_attribute('class') or ""
                                                    alt_attr = img.get_attribute('alt') or ""

                                                    # ∆Øu ti√™n ·∫£nh c√≥ class/alt ph√π h·ª£p
                                                    quality_score = 0
                                                    if any(keyword in class_attr.lower() for keyword in ['object-contain', 'rounded', 'logo', 'company']):
                                                        quality_score += 3
                                                    if any(keyword in alt_attr.lower() for keyword in ['logo', 'company', 'brand']):
                                                        quality_score += 2
                                                    if 'relative' in class_attr and 'w-full' in class_attr:
                                                        quality_score += 1

                                                    if quality_score >= 2:  # ·∫¢nh ch·∫•t l∆∞·ª£ng cao
                                                        print(f"‚úÖ Logo ch·∫•t l∆∞·ª£ng cao job {i+1}: {src[:50]}... (score: {quality_score})")
                                                        break
                                                    elif not logo_url or quality_score > 0:  # Backup t·ªët h∆°n
                                                        logo_url = src
                                                        print(f"üíº Logo backup job {i+1}: {src[:50]}... (score: {quality_score})")

                                        if logo_url and any(keyword in (logo_url + (img.get_attribute('class') or '') + (img.get_attribute('alt') or '')).lower()
                                                            for keyword in ['object-contain', 'rounded', 'logo', 'company']):
                                            break

                                    if logo_url:
                                        break

                            except Exception as selector_error:
                                continue

                        # Ph∆∞∆°ng ph√°p JavaScript backup ƒë·ªÉ t√¨m ·∫£nh
                        if not logo_url:
                            try:
                                js_logo_url = driver.execute_script("""
                                                                    const card = arguments[0];
                                                                    const images = card.querySelectorAll('img');

                                                                    // T√¨m ·∫£nh t·ªët nh·∫•t d·ª±a tr√™n class/alt/src
                                                                    let bestImg = null;
                                                                    let bestScore = 0;

                                                                    images.forEach(img => {
                                                                        let score = 0;
                                                                        const className = img.className || '';
                                                                        const alt = img.alt || '';
                                                                        const src = img.src || img.dataset.src || '';

                                                                        if (className.includes('object-contain')) score += 3;
                                                                        if (className.includes('rounded')) score += 2;
                                                                        if (className.includes('logo') || alt.toLowerCase().includes('logo')) score += 2;
                                                                        if (className.includes('company') || alt.toLowerCase().includes('company')) score += 2;
                                                                        if (className.includes('relative') && className.includes('w-full')) score += 1;

                                                                        if (src && !src.startsWith('data:') && score > bestScore) {
                                                                            bestScore = score;
                                                                            bestImg = img;
                                                                        }
                                                                    });

                                                                    return bestImg ? (bestImg.src || bestImg.dataset.src || bestImg.dataset.original) : null;
                                                                    """, job_card)

                                if js_logo_url:
                                    logo_url = js_logo_url
                                    print(f"üéØ JavaScript t√¨m th·∫•y logo job {i+1}: {js_logo_url[:50]}...")

                            except Exception as js_error:
                                pass

                        # L∆∞u k·∫øt qu·∫£
                        if logo_url and job_link:
                            # Normalize URL
                            if logo_url.startswith('//'):
                                logo_url = 'https:' + logo_url
                            elif logo_url.startswith('/'):
                                logo_url = 'https://vieclam24h.vn' + logo_url

                            job_logos[job_link] = logo_url
                            print(f"üíæ ƒê√£ l∆∞u logo job {i+1}: {logo_url[:60]}...")
                        else:
                            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y logo cho job {i+1}")

                        # Rate limiting
                        if i % 5 == 0 and i > 0:
                            time.sleep(0.5)

                    except Exception as e:
                        print(f"‚ùå L·ªói khi crawl logo job {i+1}: {e}")
                        continue

                print(f"üìä T·ªïng k·∫øt: ƒê√£ t√¨m th·∫•y logo cho {len(job_logos)}/{len(job_cards)} vi·ªác l√†m")

                listing_page_url = driver.current_url

                temp_jobs_this_page = []

                for i, job_link in enumerate(job_links, 1):
                    try:
                        print(f"\n{'*'*50}")
                        print(f"ƒêang x·ª≠ l√Ω c√¥ng vi·ªác {i}/{len(job_links)} (Trang {page_count})")
                        print(f"{'*'*50}")
                        driver.get(job_link)

                        # Ch·ªù cho ti√™u ƒë·ªÅ (h1) xu·∫•t hi·ªán ƒë·ªÉ ch·∫Øc ch·∫Øn trang ƒë√£ t·∫£i
                        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "h1")))

                        job_title, company_name, salary, location, logo_url = "Kh√¥ng x√°c ƒë·ªãnh", "Kh√¥ng x√°c ƒë·ªãnh", "Tho·∫£ thu·∫≠n", "Kh√¥ng x√°c ƒë·ªãnh", None

                        try:
                            job_title = driver.find_element(By.CSS_SELECTOR, "h1").text.strip()
                        except:
                            pass
                        try:
                            company_name = driver.find_element(By.CSS_SELECTOR,
                                                               ".box-company-info .company-name").text.strip()
                        except:
                            pass

                        # ∆Øu ti√™n s·ª≠ d·ª•ng logo t·ª´ trang danh s√°ch ƒë√£ crawl tr∆∞·ªõc
                        if job_link in job_logos:
                            logo_url = job_logos[job_link]
                        else:
                            # N·∫øu kh√¥ng c√≥, th·ª≠ t√¨m logo tr√™n trang chi ti·∫øt v·ªõi improved method
                            try:
                                # Scroll to company logo section
                                driver.execute_script("window.scrollTo(0, 200)")
                                time.sleep(0.5)

                                logo_selectors = [
                                    ".box-company-logo img",
                                    ".company-logo img",
                                    "[class*='logo'] img",
                                    ".company-info img",
                                    "img[alt*='logo' i]",
                                    "img[src*='logo' i]"
                                ]

                                for selector in logo_selectors:
                                    try:
                                        logo_element = driver.find_element(By.CSS_SELECTOR, selector)
                                        logo_candidates = [
                                            logo_element.get_attribute('src'),
                                            logo_element.get_attribute('data-src'),
                                            logo_element.get_attribute('data-original')
                                        ]

                                        for candidate in logo_candidates:
                                            if candidate and not candidate.startswith('data:'):
                                                logo_url = candidate
                                                break

                                        if logo_url:
                                            break
                                    except:
                                        continue

                            except:
                                pass

                        try:
                            salary = driver.find_element(By.CSS_SELECTOR, ".box-general-information .salary").text.strip()
                        except:
                            pass
                        try:
                            location = driver.find_element(By.CSS_SELECTOR,
                                                           ".box-general-information .address").text.strip()
                        except:
                            pass

                        job_details = get_job_details(driver)

                        # Crawl th√¥ng tin c√¥ng ty
                        company_details = {}
                        try:
                            # print("ƒêang t√¨m link ƒë·∫øn trang chi ti·∫øt c√¥ng ty...") # T·∫Øt b·ªõt log
                            wait = WebDriverWait(driver, 5)

                            company_link_selectors = [
                                ".box-company-info a[href*='danh-sach-tin-tuyen-dung-cong-ty-']",
                                "a.company-name[href*='danh-sach-tin-tuyen-dung-cong-ty-']",
                                "a[href*='danh-sach-tin-tuyen-dung-cong-ty-']"
                            ]

                            company_page_url = None
                            for selector in company_link_selectors:
                                try:
                                    company_page_link_element = wait.until(
                                        EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                                    company_page_url = company_page_link_element.get_attribute('href')
                                    if company_page_url:
                                        print("‚úÖ ƒê√£ t√¨m th·∫•y link c√¥ng ty.")
                                        break
                                except TimeoutException:
                                    continue

                            if company_page_url:
                                company_details, additional_job_links = scrape_employer_details(driver, company_page_url)
                                print("ƒê√£ crawl xong trang c√¥ng ty, quay l·∫°i trang tin tuy·ªÉn d·ª•ng.")

                                # X·ª≠ l√Ω c√°c vi·ªác l√†m b·ªï sung (gi·ªõi h·∫°n ƒë·ªÉ tr√°nh qu√° t·∫£i)
                                if additional_job_links and len(additional_job_links) <= 3:  # Gi·ªõi h·∫°n ch·ªâ 3 vi·ªác b·ªï sung
                                    print(f"T√¨m th·∫•y {len(additional_job_links)} vi·ªác l√†m b·ªï sung t·ª´ trang c√¥ng ty.")
                                    current_job_url = driver.current_url

                                    for add_job_link in additional_job_links:
                                        if add_job_link not in [job['Link'] for job in temp_jobs_this_page] and add_job_link not in job_links:
                                            try:
                                                print(f"ƒêang crawl vi·ªác l√†m b·ªï sung: {add_job_link}")
                                                driver.get(add_job_link)
                                                WebDriverWait(driver, 10).until(
                                                    EC.presence_of_element_located((By.TAG_NAME, "h1")))

                                                # Crawl th√¥ng tin c∆° b·∫£n c·ªßa vi·ªác l√†m b·ªï sung
                                                add_job_title = "Kh√¥ng x√°c ƒë·ªãnh"
                                                add_company_name = company_name
                                                add_salary = "Tho·∫£ thu·∫≠n"
                                                add_location = "Kh√¥ng x√°c ƒë·ªãnh"
                                                add_logo_url = logo_url

                                                try:
                                                    add_job_title = driver.find_element(By.CSS_SELECTOR, "h1").text.strip()
                                                except:
                                                    pass
                                                try:
                                                    add_salary = driver.find_element(By.CSS_SELECTOR, ".box-general-information .salary").text.strip()
                                                except:
                                                    pass
                                                try:
                                                    add_location = driver.find_element(By.CSS_SELECTOR, ".box-general-information .address").text.strip()
                                                except:
                                                    pass

                                                add_job_details = get_job_details(driver)

                                                # T·∫°o dict cho vi·ªác l√†m b·ªï sung
                                                additional_job_data = {
                                                    "Ti√™u ƒë·ªÅ": add_job_title,
                                                    "C√¥ng ty": add_company_name,
                                                    "Logo URL": add_logo_url,
                                                    "M·ª©c l∆∞∆°ng": add_salary,
                                                    "ƒê·ªãa ƒëi·ªÉm": add_location,
                                                    "Link": add_job_link,
                                                    "Trang": f"{page_count} (B·ªï sung)",
                                                    "Company Details": company_details,
                                                    **add_job_details
                                                }

                                                temp_jobs_this_page.append(additional_job_data)
                                                print(f"‚úÖ ƒê√£ th√™m vi·ªác l√†m b·ªï sung: {add_job_title}")

                                            except Exception as e:
                                                print(f"‚ùå L·ªói khi crawl vi·ªác l√†m b·ªï sung {add_job_link}: {e}")
                                                continue

                                    # Quay l·∫°i trang vi·ªác l√†m ch√≠nh
                                    driver.get(current_job_url)
                                    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "h1")))

                            else:
                                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y link trang c√¥ng ty.")

                        except Exception as company_error:
                            print(f"‚ùå L·ªói khi crawl th√¥ng tin c√¥ng ty: {company_error}")

                        # T·∫°o dict ch·ª©a t·∫•t c·∫£ th√¥ng tin c·ªßa c√¥ng vi·ªác ch√≠nh
                        job_data = {
                            "Ti√™u ƒë·ªÅ": job_title,
                            "C√¥ng ty": company_name,
                            "Logo URL": logo_url,
                            "M·ª©c l∆∞∆°ng": salary,
                            "ƒê·ªãa ƒëi·ªÉm": location,
                            "Link": job_link,
                            "Trang": page_count,
                            "Company Details": company_details,
                            **job_details
                        }

                        temp_jobs_this_page.append(job_data)
                        print(f"‚úÖ ƒê√£ crawl xong c√¥ng vi·ªác: {job_title}")
                        print(f"üìä T·ªïng c·ªông ƒë√£ c√≥ {len(all_jobs_data) + len(temp_jobs_this_page)} vi·ªác l√†m")

                        # Ngh·ªâ gi·ªØa c√°c request
                        time.sleep(2)

                    except Exception as job_error:
                        print(f"‚ùå L·ªói khi x·ª≠ l√Ω c√¥ng vi·ªác {job_link}: {job_error}")
                        # Quay l·∫°i trang danh s√°ch ƒë·ªÉ ti·∫øp t·ª•c
                        driver.get(listing_page_url)
                        continue

                all_jobs_data.extend(temp_jobs_this_page)

                # L∆∞u checkpoint sau m·ªói v√†i trang
                if page_count % save_checkpoint_every == 0 and temp_jobs_this_page:
                    save_checkpoint_to_db(temp_jobs_this_page, page_count)
                    all_jobs_data = [] # X√≥a checkpoint ƒë√£ l∆∞u ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ

                # Chuy·ªÉn sang trang ti·∫øp theo
                try:
                    # Quay v·ªÅ trang danh s√°ch
                    if driver.current_url != listing_page_url:
                        driver.get(listing_page_url)
                    WebDriverWait(driver, 20).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "div.grid.grid-cols-1.gap-y-2 > a")))

                    # T√¨m n√∫t "Trang ti·∫øp theo" v·ªõi nhi·ªÅu ph∆∞∆°ng ph√°p
                    next_page_found = False

                    # Ph∆∞∆°ng ph√°p 1: T√¨m n√∫t Next/Ti·∫øp theo
                    next_page_selectors = [
                        "a[aria-label='Next']",
                        "a[aria-label='next']",
                        ".pagination a.next",
                        ".pagination .next",
                        "a.next-page",
                        ".page-item.next a",
                        ".pagination-next a"
                    ]

                    for selector in next_page_selectors:
                        try:
                            next_button = driver.find_element(By.CSS_SELECTOR, selector)
                            if next_button and next_button.is_enabled() and next_button.is_displayed():
                                driver.execute_script("arguments[0].click();", next_button)
                                time.sleep(3)
                                next_page_found = True
                                print(f"‚úÖ ƒê√£ chuy·ªÉn sang trang {page_count + 1} (method: next button)")
                                break
                        except:
                            continue

                    # Ph∆∞∆°ng ph√°p 2: T√¨m text "Ti·∫øp" ho·∫∑c "Next"
                    if not next_page_found:
                        try:
                            xpath_selectors = [
                                "//a[contains(text(), 'Ti·∫øp') or contains(text(), 'ti·∫øp')]",
                                "//a[contains(text(), 'Next') or contains(text(), 'next')]",
                                "//a[contains(text(), '‚Üí') or contains(text(), '>')]"
                            ]

                            for xpath in xpath_selectors:
                                try:
                                    next_button = driver.find_element(By.XPATH, xpath)
                                    if next_button and next_button.is_enabled() and next_button.is_displayed():
                                        driver.execute_script("arguments[0].click();", next_button)
                                        time.sleep(3)
                                        next_page_found = True
                                        print(f"‚úÖ ƒê√£ chuy·ªÉn sang trang {page_count + 1} (method: text search)")
                                        break
                                except:
                                    continue

                                if next_page_found:
                                    break
                        except:
                            pass

                    # Ph∆∞∆°ng ph√°p 3: T√¨m link trang ti·∫øp theo b·∫±ng s·ªë
                    if not next_page_found:
                        try:
                            next_page_num = page_count + 1
                            page_link_selectors = [
                                f"a[href*='page={next_page_num}']",
                                f".pagination a:contains('{next_page_num}')",
                                f"a.page-link[href*='{next_page_num}']"
                            ]

                            for selector in page_link_selectors:
                                try:
                                    if ":contains" in selector:
                                        # S·ª≠ d·ª•ng XPath cho contains text
                                        next_button = driver.find_element(By.XPATH, f"//a[contains(text(), '{next_page_num}')]")
                                    else:
                                        next_button = driver.find_element(By.CSS_SELECTOR, selector)

                                    if next_button and next_button.is_enabled() and next_button.is_displayed():
                                        driver.execute_script("arguments[0].click();", next_button)
                                        time.sleep(3)
                                        next_page_found = True
                                        print(f"‚úÖ ƒê√£ chuy·ªÉn sang trang {page_count + 1} (method: page number)")
                                        break
                                except:
                                    continue

                                if next_page_found:
                                    break
                        except:
                            pass

                    # Ph∆∞∆°ng ph√°p 4: Thay ƒë·ªïi URL tr·ª±c ti·∫øp
                    if not next_page_found:
                        try:
                            current_url = driver.current_url
                            next_page_num = page_count + 1

                            # Th·ª≠ c√°c pattern URL kh√°c nhau
                            url_patterns = [
                                (f"page={page_count}", f"page={next_page_num}"),
                                (f"page-{page_count}", f"page-{next_page_num}"),
                                (f"/p{page_count}", f"/p{next_page_num}"),
                                (f"-p{page_count}", f"-p{next_page_num}")
                            ]

                            next_url = None
                            for old_pattern, new_pattern in url_patterns:
                                if old_pattern in current_url:
                                    next_url = current_url.replace(old_pattern, new_pattern)
                                    break

                            # N·∫øu kh√¥ng t√¨m th·∫•y pattern, th√™m parameter page
                            if not next_url:
                                if "page=" not in current_url:
                                    separator = "&" if "?" in current_url else "?"
                                    next_url = f"{current_url}{separator}page={next_page_num}"

                            if next_url:
                                print(f"üîÑ Th·ª≠ chuy·ªÉn trang b·∫±ng URL: {next_url}")
                                driver.get(next_url)
                                time.sleep(3)

                                # Ki·ªÉm tra xem c√≥ vi·ªác l√†m tr√™n trang m·ªõi kh√¥ng
                                try:
                                    new_job_cards = WebDriverWait(driver, 10).until(
                                        EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.grid.grid-cols-1.gap-y-2 > a"))
                                    )
                                    if new_job_cards and len(new_job_cards) > 0:
                                        next_page_found = True
                                        print(f"‚úÖ ƒê√£ chuy·ªÉn sang trang {page_count + 1} (method: URL manipulation)")
                                    else:
                                        print("‚ö†Ô∏è Trang ti·∫øp theo kh√¥ng c√≥ vi·ªác l√†m n√†o - c√≥ th·ªÉ ƒë√£ h·∫øt trang")
                                except:
                                    print("‚ö†Ô∏è Kh√¥ng th·ªÉ load trang ti·∫øp theo - c√≥ th·ªÉ ƒë√£ h·∫øt trang")
                        except Exception as url_error:
                            print(f"‚ùå L·ªói khi thay ƒë·ªïi URL: {url_error}")

                    # Ph∆∞∆°ng ph√°p 5: Ki·ªÉm tra cu·ªëi c√πng b·∫±ng c√°ch so s√°nh s·ªë vi·ªác l√†m
                    if not next_page_found:
                        try:
                            # Th·ª≠ load trang ti·∫øp theo b·∫±ng c√°ch th√™m page parameter
                            base_url = start_url.split('?')[0]  # L·∫•y base URL
                            next_page_num = page_count + 1
                            test_url = f"{base_url}?page={next_page_num}"

                            print(f"üîÑ Ki·ªÉm tra cu·ªëi c√πng v·ªõi URL: {test_url}")
                            driver.get(test_url)
                            time.sleep(3)

                            # Ki·ªÉm tra c√≥ vi·ªác l√†m kh√¥ng
                            test_job_cards = driver.find_elements(By.CSS_SELECTOR, "div.grid.grid-cols-1.gap-y-2 > a")
                            if test_job_cards and len(test_job_cards) > 0:
                                next_page_found = True
                                print(f"‚úÖ ƒê√£ chuy·ªÉn sang trang {page_count + 1} (method: base URL + page param)")
                            else:
                                print("üèÅ Kh√¥ng c√≤n trang n√†o ƒë·ªÉ crawl - ƒê√£ h·∫øt d·ªØ li·ªáu!")
                        except:
                            print("üèÅ Kh√¥ng th·ªÉ truy c·∫≠p trang ti·∫øp theo - K·∫øt th√∫c crawl")

                    if not next_page_found:
                        print("üèÅ ƒê√É CRAWL HET T·∫§T C·∫¢ C√ÅC TRANG C√ì TH·ªÇ!")
                        print(f"üìä T·ªïng c·ªông ƒë√£ crawl {page_count} trang.")
                        break

                except Exception as pagination_error:
                    print(f"‚ùå L·ªói khi chuy·ªÉn trang: {pagination_error}")
                    print("üèÅ K·∫øt th√∫c crawl do l·ªói ph√¢n trang")
                    break

                page_count += 1

            except Exception as page_error:
                print(f"‚ùå L·ªói khi x·ª≠ l√Ω trang {page_count}: {page_error}")
                break

        # L∆∞u t·∫•t c·∫£ d·ªØ li·ªáu c√≤n l·∫°i v√†o database
        print(f"\nüéâ HO√ÄN TH√ÄNH CRAWL D·ªÆ LI·ªÜU!")
        if all_jobs_data:
            print(f"üíæ ƒêang l∆∞u {len(all_jobs_data)} c√¥ng vi·ªác cu·ªëi c√πng v√†o database...")
            save_jobs_to_db(all_jobs_data)

    except Exception as main_error:
        print(f"‚ùå L·ªói ch√≠nh trong qu√° tr√¨nh crawl: {main_error}")

        # V·∫´n c·ªë g·∫Øng l∆∞u d·ªØ li·ªáu ƒë√£ crawl ƒë∆∞·ª£c v√†o database
        if all_jobs_data:
            print(f"üíæ ƒêang l∆∞u d·ªØ li·ªáu kh·∫©n c·∫•p ({len(all_jobs_data)} jobs) v√†o database...")
            save_jobs_to_db(all_jobs_data)

    finally:
        try:
            driver.quit()
            print("‚úÖ ƒê√£ ƒë√≥ng tr√¨nh duy·ªát")
        except:
            pass




def crawl_single_job(job_url):
    """H√†m crawl m·ªôt c√¥ng vi·ªác ƒë∆°n l·∫ª ƒë·ªÉ test."""
    driver = setup_edge_driver()
    if not driver:
        return None

    try:
        print(f"ƒêang crawl: {job_url}")
        driver.get(job_url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "h1")))

        # Crawl th√¥ng tin c∆° b·∫£n
        job_title = driver.find_element(By.CSS_SELECTOR, "h1").text.strip()
        company_name = driver.find_element(By.CSS_SELECTOR, ".box-company-info .company-name").text.strip()

        # Crawl logo
        logo_url = None
        try:
            logo_url = driver.find_element(By.CSS_SELECTOR, ".box-company-logo img").get_attribute('src')
        except:
            pass

        # Crawl chi ti·∫øt
        job_details = get_job_details(driver)

        print(f"‚úÖ ƒê√£ crawl: {job_title} t·∫°i {company_name}")
        job_data = {
            "Ti√™u ƒë·ªÅ": job_title,
            "C√¥ng ty": company_name,
            "Logo URL": logo_url,
            "Link": job_url,
            "Company Details": {},
            **job_details
        }

        # L∆∞u v√†o database
        if setup_database_tables():
            save_job_to_db(job_data)

        return job_data

    except Exception as e:
        print(f"‚ùå L·ªói khi crawl {job_url}: {e}")
        return None
    finally:
        driver.quit()

def test_database_connection():
    """H√†m test k·∫øt n·ªëi database."""
    print("üîç ƒêang test k·∫øt n·ªëi database...")
    connection = create_database_connection()
    if connection:
        try:
            cursor = connection.cursor()
            cursor.execute("SELECT 1")
            result = cursor.fetchone()
            if result:
                print("‚úÖ K·∫øt n·ªëi database th√†nh c√¥ng!")

                # Ki·ªÉm tra c√°c b·∫£ng
                cursor.execute("SHOW TABLES")
                tables = cursor.fetchall()
                print(f"üìã C√°c b·∫£ng hi·ªán c√≥: {[table[0] for table in tables]}")

                return True
        except Error as e:
            print(f"‚ùå L·ªói test database: {e}")
            return False
        finally:
            if connection.is_connected():
                cursor.close()
                connection.close()
    else:
        print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi database!")
        return False

def get_database_stats():
    """Hi·ªÉn th·ªã th·ªëng k√™ database."""
    connection = create_database_connection()
    if not connection:
        return

    try:
        cursor = connection.cursor()

        print("\nüìä TH·ªêNG K√ä DATABASE:")
        print("="*50)

        # Th·ªëng k√™ jobs
        cursor.execute("SELECT COUNT(*) FROM jobs")
        total_jobs = cursor.fetchone()[0]
        print(f"üìù T·ªïng s·ªë c√¥ng vi·ªác: {total_jobs}")

        # Th·ªëng k√™ companies
        cursor.execute("SELECT COUNT(*) FROM users WHERE role = 'employer'")
        total_companies = cursor.fetchone()[0]
        print(f"üè¢ T·ªïng s·ªë c√¥ng ty: {total_companies}")

        # Jobs c√≥ logo
        # S·ª¨A L·∫†I - ƒê·∫£m b·∫£o c·ªôt logo_url t·ªìn t·∫°i ƒë·ªÉ th·ªëng k√™
        cursor.execute("SELECT COUNT(*) FROM jobs WHERE logo_url IS NOT NULL AND logo_url != ''")
        jobs_with_logo = cursor.fetchone()[0]
        print(f"üñºÔ∏è  Jobs c√≥ logo: {jobs_with_logo} / {total_jobs}")

        # Jobs c√≥ company details
        cursor.execute("SELECT COUNT(*) FROM jobs WHERE company_description IS NOT NULL AND company_description != ''")
        jobs_with_company_info = cursor.fetchone()[0]
        print(f"üìã Jobs c√≥ th√¥ng tin c√¥ng ty: {jobs_with_company_info} / {total_jobs}")

        # Top companies
        cursor.execute("""
                       SELECT company_name, COUNT(*) as job_count
                       FROM jobs
                       WHERE company_name IS NOT NULL AND company_name != '' AND company_name != 'Kh√¥ng x√°c ƒë·ªãnh'
                       GROUP BY company_name
                       ORDER BY job_count DESC
                           LIMIT 5
                       """)
        top_companies = cursor.fetchall()
        print(f"\nüèÜ TOP 5 C√îNG TY C√ì NHI·ªÄU VI·ªÜC NH·∫§T:")
        for company, count in top_companies:
            print(f"   - {company}: {count} vi·ªác l√†m")

        # Top locations
        cursor.execute("""
                       SELECT location, COUNT(*) as job_count
                       FROM jobs
                       WHERE location IS NOT NULL AND location != '' AND location != 'Kh√¥ng x√°c ƒë·ªãnh'
                       GROUP BY location
                       ORDER BY job_count DESC
                           LIMIT 5
                       """)
        top_locations = cursor.fetchall()
        print(f"\nüìç TOP 5 ƒê·ªäA ƒêI·ªÇM C√ì NHI·ªÄU VI·ªÜC NH·∫§T:")
        for location, count in top_locations:
            print(f"   - {location}: {count} vi·ªác l√†m")

        print("="*50)

    except Error as e:
        print(f"‚ùå L·ªói khi l·∫•y th·ªëng k√™: {e}")
    finally:
        if connection.is_connected():
            cursor.close()
            connection.close()


# ==============================================================================
# PH·∫¶N CH·∫†Y CH∆Ø∆†NG TR√åNH
# ==============================================================================

if __name__ == "__main__":
    print("üöÄ B·∫ÆT ƒê·∫¶U CRAWL D·ªÆ LI·ªÜU VIECLAM24H.VN - MYSQL DATABASE VERSION")
    print("="*70)

    # L·ªùi khuy√™n: N·∫øu b·∫°n g·∫∑p l·ªói "Unknown column", h√£y x√≥a b·∫£ng 'jobs' c≈© trong database
    # ƒë·ªÉ script c√≥ th·ªÉ t·∫°o l·∫°i b·∫£ng m·ªõi v·ªõi c·∫•u tr√∫c ƒë√∫ng.
    # L·ªánh SQL ƒë·ªÉ x√≥a b·∫£ng: DROP TABLE jobs;

    # Test database tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu
    print("üîß KI·ªÇM TRA CHU·∫®N B·ªä:")
    print("-"*30)

    if not test_database_connection():
        print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi database. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh DB_CONFIG.")
        print("üìù C·∫ßn t·∫°o database 'job_finder_app' v√† c·∫≠p nh·∫≠t th√¥ng tin k·∫øt n·ªëi.")
        exit()

    # Setup database tables
    if not setup_database_tables():
        print("‚ùå Kh√¥ng th·ªÉ thi·∫øt l·∫≠p database tables.")
        exit()

    # Hi·ªÉn th·ªã th·ªëng k√™ hi·ªán t·∫°i
    get_database_stats()

    # C·∫•u h√¨nh crawl
    MAX_PAGES = None  # None = crawl h·∫øt t·∫•t c·∫£ trang
    SAVE_CHECKPOINT_EVERY = 2  # L∆∞u checkpoint sau m·ªói 3 trang

    print(f"\nüìã C·∫§U H√åNH CRAWL:")
    print("-"*30)
    if MAX_PAGES:
        print(f"- S·ªë trang t·ªëi ƒëa: {MAX_PAGES}")
    else:
        print(f"- S·ªë trang t·ªëi ƒëa: KH√îNG GI·ªöI H·∫†N (crawl h·∫øt t·∫•t c·∫£)")
    print(f"- L∆∞u checkpoint m·ªói: {SAVE_CHECKPOINT_EVERY} trang")
    print(f"- Ngu·ªìn: vieclam24h.vn (IT Software)")
    print(f"- WebDriver: Microsoft Edge")
    print(f"- Driver Path: D:\\EdgeDriver\\msedgedriver.exe")
    print(f"- Database: {DB_CONFIG['database']} @ {DB_CONFIG['host']}")
    print(f"- CRAWL LOGO: ‚úÖ (class: relative.w-full.h-full.object-contain.my-auto.rounded-md)")
    print("="*70)

    # X√°c nh·∫≠n t·ª´ ng∆∞·ªùi d√πng
    try:
        confirm = input("\n‚ö†Ô∏è  B·∫†N MU·ªêN B·∫ÆT ƒê·∫¶U CRAWL? (y/n): ").lower().strip()
        if confirm not in ['y', 'yes', 'c√≥', 'ok']:
            print("‚ùå ƒê√£ h·ªßy crawl.")
            exit()
    except:
        # N·∫øu kh√¥ng th·ªÉ input (nh∆∞ tr√™n Kaggle), t·ª± ƒë·ªông ch·∫°y
        print("ü§ñ T·ª± ƒë·ªông ch·∫°y crawl (m√¥i tr∆∞·ªùng kh√¥ng h·ªó tr·ª£ input)")

    # B·∫Øt ƒë·∫ßu crawl
    print("\nüöÄ B·∫ÆT ƒê·∫¶U CRAWL...")
    crawl_vieclam24h_edge(max_pages=MAX_PAGES, save_checkpoint_every=SAVE_CHECKPOINT_EVERY)

    # Hi·ªÉn th·ªã th·ªëng k√™ cu·ªëi c√πng
    print("\nüìä TH·ªêNG K√ä CU·ªêI C√ôNG:")
    get_database_stats()

    print("\nüéØ HO√ÄN TH√ÄNH CH∆Ø∆†G TR√åNH!")
    print("Ki·ªÉm tra database 'job_finder_app' ƒë·ªÉ xem d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u.")